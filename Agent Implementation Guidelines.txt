OpenAI Agents SDK The OpenAI Agents SDK enables you to build agentic AI apps in a lightweight, easy-to-use package with very few abstractions. It's a production-ready upgrade of our previous experimentation for agents, Swarm. The Agents SDK has a very small set of primitives: Agents, which are LLMs equipped with instructions and tools Handoffs, which allow agents to delegate to other agents for specific tasks Guardrails, which enable the inputs to agents to be validated In combination with Python, these primitives are powerful enough to express complex relationships between tools and agents, and allow you to build real-world applications without a steep learning curve. In addition, the SDK comes with built-in tracing that lets you visualize and debug your agentic flows, as well as evaluate them and even fine-tune models for your application. Why use the Agents SDK The SDK has two driving design principles: Enough features to be worth using, but few enough primitives to make it quick to learn. Works great out of the box, but you can customize exactly what happens. Here are the main features of the SDK: Agent loop: Built-in agent loop that handles calling tools, sending results to the LLM, and looping until the LLM is done. Python-first: Use built-in language features to orchestrate and chain agents, rather than needing to learn new abstractions. Handoffs: A powerful feature to coordinate and delegate between multiple agents. Guardrails: Run input validations and checks in parallel to your agents, breaking early if the checks fail. Function tools: Turn any Python function into a tool, with automatic schema generation and Pydantic-powered validation. Tracing: Built-in tracing that lets you visualize, debug and monitor your workflows, as well as use the OpenAI suite of evaluation, fine-tuning and distillation tools. Installation pip install openai-agents Hello world example from agents import Agent, Runner agent = Agent(name="Assistant", instructions="You are a helpful assistant") result = Runner.run_sync(agent, "Write a haiku about recursion in programming.") print(result.final_output) # Code within the code, # Functions calling themselves, # Infinite loop's dance. (If running this, ensure you set the OPENAI_API_KEY environment variable) export OPENAI_API_KEY=sk-... Quickstart Create a project and virtual environment You'll only need to do this once. mkdir my_project cd my_project python -m venv .venv Activate the virtual environment Do this every time you start a new terminal session. source .venv/bin/activate Install the Agents SDK pip install openai-agents # or uv add openai-agents, etc Set an OpenAI API key If you don't have one, follow these instructions to create an OpenAI API key. export OPENAI_API_KEY=sk-... Create your first agent Agents are defined with instructions, a name, and optional config (such as model_config) from agents import Agent agent = Agent( name="Math Tutor", instructions="You provide help with math problems. Explain your reasoning at each step and include examples", ) Add a few more agents Additional agents can be defined in the same way. handoff_descriptions provide additional context for determining handoff routing from agents import Agent history_tutor_agent = Agent( name="History Tutor", handoff_description="Specialist agent for historical questions", instructions="You provide assistance with historical queries. Explain important events and context clearly.", ) math_tutor_agent = Agent( name="Math Tutor", handoff_description="Specialist agent for math questions", instructions="You provide help with math problems. Explain your reasoning at each step and include examples", ) Define your handoffs On each agent, you can define an inventory of outgoing handoff options that the agent can choose from to decide how to make progress on their task. triage_agent = Agent( name="Triage Agent", instructions="You determine which agent to use based on the user's homework question", handoffs=[history_tutor_agent, math_tutor_agent] ) Run the agent orchestration Let's check that the workflow runs and the triage agent correctly routes between the two specialist agents. from agents import Runner async def main(): result = await Runner.run(triage_agent, "What is the capital of France?") print(result.final_output) Add a guardrail You can define custom guardrails to run on the input or output. from agents import GuardrailFunctionOutput, Agent, Runner from pydantic import BaseModel class HomeworkOutput(BaseModel): is_homework: bool reasoning: str guardrail_agent = Agent( name="Guardrail check", instructions="Check if the user is asking about homework.", output_type=HomeworkOutput, ) async def homework_guardrail(ctx, agent, input_data): result = await Runner.run(guardrail_agent, input_data, context=ctx.context) final_output = result.final_output_as(HomeworkOutput) return GuardrailFunctionOutput( output_info=final_output, tripwire_triggered=not final_output.is_homework, ) Put it all together Let's put it all together and run the entire workflow, using handoffs and the input guardrail. from agents import Agent, InputGuardrail,GuardrailFunctionOutput, Runner from pydantic import BaseModel import asyncio class HomeworkOutput(BaseModel): is_homework: bool reasoning: str guardrail_agent = Agent( name="Guardrail check", instructions="Check if the user is asking about homework.", output_type=HomeworkOutput, ) math_tutor_agent = Agent( name="Math Tutor", handoff_description="Specialist agent for math questions", instructions="You provide help with math problems. Explain your reasoning at each step and include examples", ) history_tutor_agent = Agent( name="History Tutor", handoff_description="Specialist agent for historical questions", instructions="You provide assistance with historical queries. Explain important events and context clearly.", ) async def homework_guardrail(ctx, agent, input_data): result = await Runner.run(guardrail_agent, input_data, context=ctx.context) final_output = result.final_output_as(HomeworkOutput) return GuardrailFunctionOutput( output_info=final_output, tripwire_triggered=not final_output.is_homework, ) triage_agent = Agent( name="Triage Agent", instructions="You determine which agent to use based on the user's homework question", handoffs=[history_tutor_agent, math_tutor_agent], input_guardrails=[ InputGuardrail(guardrail_function=homework_guardrail), ], ) async def main(): result = await Runner.run(triage_agent, "who was the first president of the united states?") print(result.final_output) result = await Runner.run(triage_agent, "what is life") print(result.final_output) if __name__ == "__main__": asyncio.run(main()) View your traces To review what happened during your agent run, navigate to the Trace viewer in the OpenAI Dashboard to view traces of your agent runs. Next steps Learn how to build more complex agentic flows: Learn about how to configure Agents. Learn about running agents. Learn about tools, guardrails and models. Agents Agents are the core building block in your apps. An agent is a large language model (LLM), configured with instructions and tools. Basic configuration The most common properties of an agent you'll configure are: instructions: also known as a developer message or system prompt. model: which LLM to use, and optional model_settings to configure model tuning parameters like temperature, top_p, etc. tools: Tools that the agent can use to achieve its tasks. from agents import Agent, ModelSettings, function_tool def get_weather(city: str) -> str: return f"The weather in {city} is sunny" agent = Agent( name="Haiku agent", instructions="Always respond in haiku form", model="o3-mini", tools=[function_tool(get_weather)], ) Context Agents are generic on their context type. Context is a dependency-injection tool: it's an object you create and pass to Runner.run(), that is passed to every agent, tool, handoff etc, and it serves as a grab bag of dependencies and state for the agent run. You can provide any Python object as the context. @dataclass class UserContext: uid: str is_pro_user: bool async def fetch_purchases() -> list[Purchase]: return ... agent = Agent[UserContext]( ..., ) Output types By default, agents produce plain text (i.e. str) outputs. If you want the agent to produce a particular type of output, you can use the output_type parameter. A common choice is to use Pydantic objects, but we support any type that can be wrapped in a Pydantic TypeAdapter - dataclasses, lists, TypedDict, etc. from pydantic import BaseModel from agents import Agent class CalendarEvent(BaseModel): name: str date: str participants: list[str] agent = Agent( name="Calendar extractor", instructions="Extract calendar events from text", output_type=CalendarEvent, ) Note When you pass an output_type, that tells the model to use structured outputs instead of regular plain text responses. Handoffs Handoffs are sub-agents that the agent can delegate to. You provide a list of handoffs, and the agent can choose to delegate to them if relevant. This is a powerful pattern that allows orchestrating modular, specialized agents that excel at a single task. Read more in the handoffs documentation. from agents import Agent booking_agent = Agent(...) refund_agent = Agent(...) triage_agent = Agent( name="Triage agent", instructions=( "Help the user with their questions." "If they ask about booking, handoff to the booking agent." "If they ask about refunds, handoff to the refund agent." ), handoffs=[booking_agent, refund_agent], ) Dynamic instructions In most cases, you can provide instructions when you create the agent. However, you can also provide dynamic instructions via a function. The function will receive the agent and context, and must return the prompt. Both regular and async functions are accepted. def dynamic_instructions( context: RunContextWrapper[UserContext], agent: Agent[UserContext] ) -> str: return f"The user's name is {context.context.name}. Help them with their questions." agent = Agent[UserContext]( name="Triage agent", instructions=dynamic_instructions, ) Lifecycle events (hooks) Sometimes, you want to observe the lifecycle of an agent. For example, you may want to log events, or pre-fetch data when certain events occur. You can hook into the agent lifecycle with the hooks property. Subclass the AgentHooks class, and override the methods you're interested in. Guardrails Guardrails allow you to run checks/validations on user input, in parallel to the agent running. For example, you could screen the user's input for relevance. Read more in the guardrails documentation. Cloning/copying agents By using the clone() method on an agent, you can duplicate an Agent, and optionally change any properties you like. pirate_agent = Agent( name="Pirate", instructions="Write like a pirate", model="o3-mini", ) robot_agent = pirate_agent.clone( name="Robot", instructions="Write like a robot", ) Running agents You can run agents via the Runner class. You have 3 options: Runner.run(), which runs async and returns a RunResult. Runner.run_sync(), which is a sync method and just runs .run() under the hood. Runner.run_streamed(), which runs async and returns a RunResultStreaming. It calls the LLM in streaming mode, and streams those events to you as they are received. from agents import Agent, Runner async def main(): agent = Agent(name="Assistant", instructions="You are a helpful assistant") result = await Runner.run(agent, "Write a haiku about recursion in programming.") print(result.final_output) # Code within the code, # Functions calling themselves, # Infinite loop's dance. Read more in the results guide. The agent loop When you use the run method in Runner, you pass in a starting agent and input. The input can either be a string (which is considered a user message), or a list of input items, which are the items in the OpenAI Responses API. The runner then runs a loop: We call the LLM for the current agent, with the current input. The LLM produces its output. If the LLM returns a final_output, the loop ends and we return the result. If the LLM does a handoff, we update the current agent and input, and re-run the loop. If the LLM produces tool calls, we run those tool calls, append the results, and re-run the loop. If we exceed the max_turns passed, we raise a MaxTurnsExceeded exception. Note The rule for whether the LLM output is considered as a "final output" is that it produces text output with the desired type, and there are no tool calls. Streaming Streaming allows you to additionally receive streaming events as the LLM runs. Once the stream is done, the RunResultStreaming will contain the complete information about the run, including all the new outputs produces. You can call .stream_events() for the streaming events. Read more in the streaming guide. Run config The run_config parameter lets you configure some global settings for the agent run: model: Allows setting a global LLM model to use, irrespective of what model each Agent has. model_provider: A model provider for looking up model names, which defaults to OpenAI. model_settings: Overrides agent-specific settings. For example, you can set a global temperature or top_p. input_guardrails, output_guardrails: A list of input or output guardrails to include on all runs. handoff_input_filter: A global input filter to apply to all handoffs, if the handoff doesn't already have one. The input filter allows you to edit the inputs that are sent to the new agent. See the documentation in Handoff.input_filter for more details. tracing_disabled: Allows you to disable tracing for the entire run. trace_include_sensitive_data: Configures whether traces will include potentially sensitive data, such as LLM and tool call inputs/outputs. workflow_name, trace_id, group_id: Sets the tracing workflow name, trace ID and trace group ID for the run. We recommend at least setting workflow_name. The session ID is an optional field that lets you link traces across multiple runs. trace_metadata: Metadata to include on all traces. Conversations/chat threads Calling any of the run methods can result in one or more agents running (and hence one or more LLM calls), but it represents a single logical turn in a chat conversation. For example: User turn: user enter text Runner run: first agent calls LLM, runs tools, does a handoff to a second agent, second agent runs more tools, and then produces an output. At the end of the agent run, you can choose what to show to the user. For example, you might show the user every new item generated by the agents, or just the final output. Either way, the user might then ask a followup question, in which case you can call the run method again. You can use the base RunResultBase.to_input_list() method to get the inputs for the next turn. async def main(): agent = Agent(name="Assistant", instructions="Reply very concisely.") with trace(workflow_name="Conversation", group_id=thread_id): # First turn result = await Runner.run(agent, "What city is the Golden Gate Bridge in?") print(result.final_output) # San Francisco # Second turn new_input = output.to_input_list() + [{"role": "user", "content": "What state is it in?"}] result = await Runner.run(agent, new_input) print(result.final_output) # California Exceptions The SDK raises exceptions in certain cases. The full list is in agents.exceptions. As an overview: AgentsException is the base class for all exceptions raised in the SDK. MaxTurnsExceeded is raised when the run exceeds the max_turns passed to the run methods. ModelBehaviorError is raised when the model produces invalid outputs, e.g. malformed JSON or using non-existent tools. UserError is raised when you (the person writing code using the SDK) make an error using the SDK. InputGuardrailTripwireTriggered, OutputGuardrailTripwireTriggered is raised when a guardrail is tripped. Results When you call the Runner.run methods, you either get a: RunResult if you call run or run_sync RunResultStreaming if you call run_streamed Both of these inherit from RunResultBase, which is where most useful information is present. Final output The final_output property contains the final output of the last agent that ran. This is either: a str, if the last agent didn't have an output_type defined an object of type last_agent.output_type, if the agent had an output type defined. Note final_output is of type Any. We can't statically type this, because of handoffs. If handoffs occur, that means any Agent might be the last agent, so we don't statically know the set of possible output types. Inputs for the next turn You can use result.to_input_list() to turn the result into an input list that concatenates the original input you provided, to the items generated during the agent run. This makes it convenient to take the outputs of one agent run and pass them into another run, or to run it in a loop and append new user inputs each time. Last agent The last_agent property contains the last agent that ran. Depending on your application, this is often useful for the next time the user inputs something. For example, if you have a frontline triage agent that hands off to a language-specific agent, you can store the last agent, and re-use it the next time the user messages the agent. New items The new_items property contains the new items generated during the run. The items are RunItems. A run item wraps the raw item generated by the LLM. MessageOutputItem indicates a message from the LLM. The raw item is the message generated. HandoffCallItem indicates that the LLM called the handoff tool. The raw item is the tool call item from the LLM. HandoffOutputItem indicates that a handoff occurred. The raw item is the tool response to the handoff tool call. You can also access the source/target agents from the item. ToolCallItem indicates that the LLM invoked a tool. ToolCallOutputItem indicates that a tool was called. The raw item is the tool response. You can also access the tool output from the item. ReasoningItem indicates a reasoning item from the LLM. The raw item is the reasoning generated. Other information Guardrail results The input_guardrail_results and output_guardrail_results properties contain the results of the guardrails, if any. Guardrail results can sometimes contain useful information you want to log or store, so we make these available to you. Raw responses The raw_responses property contains the ModelResponses generated by the LLM. Original input The input property contains the original input you provided to the run method. In most cases you won't need this, but it's available in case you do. Streaming Streaming lets you subscribe to updates of the agent run as it proceeds. This can be useful for showing the end-user progress updates and partial responses. To stream, you can call Runner.run_streamed(), which will give you a RunResultStreaming. Calling result.stream_events() gives you an async stream of StreamEvent objects, which are described below. Raw response events RawResponsesStreamEvent are raw events passed directly from the LLM. They are in OpenAI Responses API format, which means each event has a type (like response.created, response.output_text.delta, etc) and data. These events are useful if you want to stream response messages to the user as soon as they are generated. For example, this will output the text generated by the LLM token-by-token. import asyncio from openai.types.responses import ResponseTextDeltaEvent from agents import Agent, Runner async def main(): agent = Agent( name="Joker", instructions="You are a helpful assistant.", ) result = Runner.run_streamed(agent, input="Please tell me 5 jokes.") async for event in result.stream_events(): if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent): print(event.data.delta, end="", flush=True) if __name__ == "__main__": asyncio.run(main()) Run item events and agent events RunItemStreamEvents are higher level events. They inform you when an item has been fully generated. This allows you to push progress updates at the level of "message generated", "tool ran", etc, instead of each token. Similarly, AgentUpdatedStreamEvent gives you updates when the current agent changes (e.g. as the result of a handoff). For example, this will ignore raw events and stream updates to the user. import asyncio import random from agents import Agent, ItemHelpers, Runner, function_tool @function_tool def how_many_jokes() -> int: return random.randint(1, 10) async def main(): agent = Agent( name="Joker", instructions="First call the how_many_jokes tool, then tell that many jokes.", tools=[how_many_jokes], ) result = Runner.run_streamed( agent, input="Hello", ) print("=== Run starting ===") async for event in result.stream_events(): # We'll ignore the raw responses event deltas if event.type == "raw_response_event": continue # When the agent updates, print that elif event.type == "agent_updated_stream_event": print(f"Agent updated: {event.new_agent.name}") continue # When items are generated, print them elif event.type == "run_item_stream_event": if event.item.type == "tool_call_item": print("-- Tool was called") elif event.item.type == "tool_call_output_item": print(f"-- Tool output: {event.item.output}") elif event.item.type == "message_output_item": print(f"-- Message output:\n {ItemHelpers.text_message_output(event.item)}") else: pass # Ignore other event types print("=== Run complete ===") if __name__ == "__main__": asyncio.run(main()) Tools Tools let agents take actions: things like fetching data, running code, calling external APIs, and even using a computer. There are three classes of tools in the Agent SDK: Hosted tools: these run on LLM servers alongside the AI models. OpenAI offers retrieval, web search and computer use as hosted tools. Function calling: these allow you to use any Python function as a tool. Agents as tools: this allows you to use an agent as a tool, allowing Agents to call other agents without handing off to them. Hosted tools OpenAI offers a few built-in tools when using the OpenAIResponsesModel: The WebSearchTool lets an agent search the web. The FileSearchTool allows retrieving information from your OpenAI Vector Stores. The ComputerTool allows automating computer use tasks. from agents import Agent, FileSearchTool, Runner, WebSearchTool agent = Agent( name="Assistant", tools=[ WebSearchTool(), FileSearchTool( max_num_results=3, vector_store_ids=["VECTOR_STORE_ID"], ), ], ) async def main(): result = await Runner.run(agent, "Which coffee shop should I go to, taking into account my preferences and the weather today in SF?") print(result.final_output) Function tools You can use any Python function as a tool. The Agents SDK will setup the tool automatically: The name of the tool will be the name of the Python function (or you can provide a name) Tool description will be taken from the docstring of the function (or you can provide a description) The schema for the function inputs is automatically created from the function's arguments Descriptions for each input are taken from the docstring of the function, unless disabled We use Python's inspect module to extract the function signature, along with griffe to parse docstrings and pydantic for schema creation. import json from typing_extensions import TypedDict, Any from agents import Agent, FunctionTool, RunContextWrapper, function_tool class Location(TypedDict): lat: float long: float @function_tool async def fetch_weather(location: Location) -> str: """Fetch the weather for a given location. Args: location: The location to fetch the weather for. """ # In real life, we'd fetch the weather from a weather API return "sunny" @function_tool(name_override="fetch_data") def read_file(ctx: RunContextWrapper[Any], path: str, directory: str | None = None) -> str: """Read the contents of a file. Args: path: The path to the file to read. directory: The directory to read the file from. """ # In real life, we'd read the file from the file system return "<file contents>" agent = Agent( name="Assistant", tools=[fetch_weather, read_file], ) for tool in agent.tools: if isinstance(tool, FunctionTool): print(tool.name) print(tool.description) print(json.dumps(tool.params_json_schema, indent=2)) print() Expand to see output Custom function tools Sometimes, you don't want to use a Python function as a tool. You can directly create a FunctionTool if you prefer. You'll need to provide: name description params_json_schema, which is the JSON schema for the arguments on_invoke_tool, which is an async function that receives the context and the arguments as a JSON string, and must return the tool output as a string. from typing import Any from pydantic import BaseModel from agents import RunContextWrapper, FunctionTool def do_some_work(data: str) -> str: return "done" class FunctionArgs(BaseModel): username: str age: int async def run_function(ctx: RunContextWrapper[Any], args: str) -> str: parsed = FunctionArgs.model_validate_json(args) return do_some_work(data=f"{parsed.username} is {parsed.age} years old") tool = FunctionTool( name="process_user", description="Processes extracted user data", params_json_schema=FunctionArgs.model_json_schema(), on_invoke_tool=run_function, ) Automatic argument and docstring parsing As mentioned before, we automatically parse the function signature to extract the schema for the tool, and we parse the docstring to extract descriptions for the tool and for individual arguments. Some notes on that: The signature parsing is done via the inspect module. We use type annotations to understand the types for the arguments, and dynamically build a Pydantic model to represent the overall schema. It supports most types, including Python primitives, Pydantic models, TypedDicts, and more. We use griffe to parse docstrings. Supported docstring formats are google, sphinx and numpy. We attempt to automatically detect the docstring format, but this is best-effort and you can explicitly set it when calling function_tool. You can also disable docstring parsing by setting use_docstring_info to False. The code for the schema extraction lives in agents.function_schema. Agents as tools In some workflows, you may want a central agent to orchestrate a network of specialized agents, instead of handing off control. You can do this by modeling agents as tools. from agents import Agent, Runner import asyncio spanish_agent = Agent( name="Spanish agent", instructions="You translate the user's message to Spanish", ) french_agent = Agent( name="French agent", instructions="You translate the user's message to French", ) orchestrator_agent = Agent( name="orchestrator_agent", instructions=( "You are a translation agent. You use the tools given to you to translate." "If asked for multiple translations, you call the relevant tools." ), tools=[ spanish_agent.as_tool( tool_name="translate_to_spanish", tool_description="Translate the user's message to Spanish", ), french_agent.as_tool( tool_name="translate_to_french", tool_description="Translate the user's message to French", ), ], ) async def main(): result = await Runner.run(orchestrator_agent, input="Say 'Hello, how are you?' in Spanish.") print(result.final_output) Handling errors in function tools When you create a function tool via @function_tool, you can pass a failure_error_function. This is a function that provides an error response to the LLM in case the tool call crashes. By default (i.e. if you don't pass anything), it runs a default_tool_error_function which tells the LLM an error occurred. If you pass your own error function, it runs that instead, and sends the response to the LLM. If you explicitly pass None, then any tool call errors will be re-raised for you to handle. This could be a ModelBehaviorError if the model produced invalid JSON, or a UserError if your code crashed, etc. If you are manually creating a FunctionTool object, then you must handle errors inside the on_invoke_tool function. Handoffs Handoffs allow an agent to delegate tasks to another agent. This is particularly useful in scenarios where different agents specialize in distinct areas. For example, a customer support app might have agents that each specifically handle tasks like order status, refunds, FAQs, etc. Handoffs are represented as tools to the LLM. So if there's a handoff to an agent named Refund Agent, the tool would be called transfer_to_refund_agent. Creating a handoff All agents have a handoffs param, which can either take an Agent directly, or a Handoff object that customizes the Handoff. You can create a handoff using the handoff() function provided by the Agents SDK. This function allows you to specify the agent to hand off to, along with optional overrides and input filters. Basic Usage Here's how you can create a simple handoff: from agents import Agent, handoff billing_agent = Agent(name="Billing agent") refund_agent = Agent(name="Refund agent") triage_agent = Agent(name="Triage agent", handoffs=[billing_agent, handoff(refund_agent)]) Customizing handoffs via the handoff() function The handoff() function lets you customize things. agent: This is the agent to which things will be handed off. tool_name_override: By default, the Handoff.default_tool_name() function is used, which resolves to transfer_to_<agent_name>. You can override this. tool_description_override: Override the default tool description from Handoff.default_tool_description() on_handoff: A callback function executed when the handoff is invoked. This is useful for things like kicking off some data fetching as soon as you know a handoff is being invoked. This function receives the agent context, and can optionally also receive LLM generated input. The input data is controlled by the input_type param. input_type: The type of input expected by the handoff (optional). input_filter: This lets you filter the input received by the next agent. See below for more. from agents import Agent, handoff, RunContextWrapper def on_handoff(ctx: RunContextWrapper[None]): print("Handoff called") agent = Agent(name="My agent") handoff_obj = handoff( agent=agent, on_handoff=on_handoff, tool_name_override="custom_handoff_tool", tool_description_override="Custom description", ) Handoff inputs In certain situations, you want the LLM to provide some data when it calls a handoff. For example, imagine a handoff to an "Escalation agent". You might want a reason to be provided, so you can log it. from pydantic import BaseModel from agents import Agent, handoff, RunContextWrapper class EscalationData(BaseModel): reason: str async def on_handoff(ctx: RunContextWrapper[None], input_data: EscalationData): print(f"Escalation agent called with reason: {input_data.reason}") agent = Agent(name="Escalation agent") handoff_obj = handoff( agent=agent, on_handoff=on_handoff, input_type=EscalationData, ) Input filters When a handoff occurs, it's as though the new agent takes over the conversation, and gets to see the entire previous conversation history. If you want to change this, you can set an input_filter. An input filter is a function that receives the existing input via a HandoffInputData, and must return a new HandoffInputData. There are some common patterns (for example removing all tool calls from the history), which are implemented for you in agents.extensions.handoff_filters from agents import Agent, handoff from agents.extensions import handoff_filters agent = Agent(name="FAQ agent") handoff_obj = handoff( agent=agent, input_filter=handoff_filters.remove_all_tools, ) Recommended prompts To make sure that LLMs understand handoffs properly, we recommend including information about handoffs in your agents. We have a suggested prefix in agents.extensions.handoff_prompt.RECOMMENDED_PROMPT_PREFIX, or you can call agents.extensions.handoff_prompt.prompt_with_handoff_instructions to automatically add recommended data to your prompts. from agents import Agent from agents.extensions.handoff_prompt import RECOMMENDED_PROMPT_PREFIX billing_agent = Agent( name="Billing agent", instructions=f"""{RECOMMENDED_PROMPT_PREFIX} <Fill in the rest of your prompt here>.""", ) Tracing The Agents SDK includes built-in tracing, collecting a comprehensive record of events during an agent run: LLM generations, tool calls, handoffs, guardrails, and even custom events that occur. Using the Traces dashboard, you can debug, visualize, and monitor your workflows during development and in production. Note Tracing is enabled by default. There are two ways to disable tracing: You can globally disable tracing by setting the env var OPENAI_AGENTS_DISABLE_TRACING=1 You can disable tracing for a single run by setting agents.run.RunConfig.tracing_disabled to True Traces and spans Traces represent a single end-to-end operation of a "workflow". They're composed of Spans. Traces have the following properties: workflow_name: This is the logical workflow or app. For example "Code generation" or "Customer service". trace_id: A unique ID for the trace. Automatically generated if you don't pass one. Must have the format trace_<32_alphanumeric>. group_id: Optional group ID, to link multiple traces from the same conversation. For example, you might use a chat thread ID. disabled: If True, the trace will not be recorded. metadata: Optional metadata for the trace. Spans represent operations that have a start and end time. Spans have: started_at and ended_at timestamps. trace_id, to represent the trace they belong to parent_id, which points to the parent Span of this Span (if any) span_data, which is information about the Span. For example, AgentSpanData contains information about the Agent, GenerationSpanData contains information about the LLM generation, etc. Default tracing By default, the SDK traces the following: The entire Runner.{run, run_sync, run_streamed}() is wrapped in a trace(). Each time an agent runs, it is wrapped in agent_span() LLM generations are wrapped in generation_span() Function tool calls are each wrapped in function_span() Guardrails are wrapped in guardrail_span() Handoffs are wrapped in handoff_span() By default, the trace is named "Agent trace". You can set this name if you use trace, or you can can configure the name and other properties with the RunConfig. In addition, you can set up custom trace processors to push traces to other destinations (as a replacement, or secondary destination). Higher level traces Sometimes, you might want multiple calls to run() to be part of a single trace. You can do this by wrapping the entire code in a trace(). from agents import Agent, Runner, trace async def main(): agent = Agent(name="Joke generator", instructions="Tell funny jokes.") with trace("Joke workflow"): first_result = await Runner.run(agent, "Tell me a joke") second_result = await Runner.run(agent, f"Rate this joke: {first_output.final_output}") print(f"Joke: {first_result.final_output}") print(f"Rating: {second_result.final_output}") Creating traces You can use the trace() function to create a trace. Traces need to be started and finished. You have two options to do so: Recommended: use the trace as a context manager, i.e. with trace(...) as my_trace. This will automatically start and end the trace at the right time. You can also manually call trace.start() and trace.finish(). The current trace is tracked via a Python contextvar. This means that it works with concurrency automatically. If you manually start/end a trace, you'll need to pass mark_as_current and reset_current to start()/finish() to update the current trace. Creating spans You can use the various *_span() methods to create a span. In general, you don't need to manually create spans. A custom_span() function is available for tracking custom span information. Spans are automatically part of the current trace, and are nested under the nearest current span, which is tracked via a Python contextvar. Sensitive data Some spans track potentially sensitive data. For example, the generation_span() stores the inputs/outputs of the LLM generation, and function_span() stores the inputs/outputs of function calls. These may contain sensitive data, so you can disable capturing that data via RunConfig.trace_include_sensitive_data. Custom tracing processors The high level architecture for tracing is: At initialization, we create a global TraceProvider, which is responsible for creating traces. We configure the TraceProvider with a BatchTraceProcessor that sends traces/spans in batches to a BackendSpanExporter, which exports the spans and traces to the OpenAI backend in batches. To customize this default setup, to send traces to alternative or additional backends or modifying exporter behavior, you have two options: add_trace_processor() lets you add an additional trace processor that will receive traces and spans as they are ready. This lets you do your own processing in addition to sending traces to OpenAI's backend. set_trace_processors() lets you replace the default processors with your own trace processors. This means traces will not be sent to the OpenAI backend unless you include a TracingProcessor that does so. External trace processors include: Braintrust Pydantic Logfire AgentOps Context management Context is an overloaded term. There are two main classes of context you might care about: Context available locally to your code: this is data and dependencies you might need when tool functions run, during callbacks like on_handoff, in lifecycle hooks, etc. Context available to LLMs: this is data the LLM sees when generating a response. Local context This is represented via the RunContextWrapper class and the context property within it. The way this works is: You create any Python object you want. A common pattern is to use a dataclass or a Pydantic object. You pass that object to the various run methods (e.g. Runner.run(..., **context=whatever**)). All your tool calls, lifecycle hooks etc will be passed a wrapper object, RunContextWrapper[T], where T represents your context object type which you can access via wrapper.context. The most important thing to be aware of: every agent, tool function, lifecycle etc for a given agent run must use the same type of context. You can use the context for things like: Contextual data for your run (e.g. things like a username/uid or other information about the user) Dependencies (e.g. logger objects, data fetchers, etc) Helper functions Note The context object is not sent to the LLM. It is purely a local object that you can read from, write to and call methods on it. import asyncio from dataclasses import dataclass from agents import Agent, RunContextWrapper, Runner, function_tool @dataclass class UserInfo: name: str uid: int async def fetch_user_age(wrapper: RunContextWrapper[UserInfo]) -> str: return f"User {wrapper.context.name} is 47 years old" async def main(): user_info = UserInfo(name="John", uid=123) agent = Agent[UserInfo]( name="Assistant", tools=[function_tool(fetch_user_age)], ) result = await Runner.run( starting_agent=agent, input="What is the age of the user?", context=user_info, ) print(result.final_output) # The user John is 47 years old. if __name__ == "__main__": asyncio.run(main()) Agent/LLM context When an LLM is called, the only data it can see is from the conversation history. This means that if you want to make some new data available to the LLM, you must do it in a way that makes it available in that history. There are a few ways to do this: You can add it to the Agent instructions. This is also known as a "system prompt" or "developer message". System prompts can be static strings, or they can be dynamic functions that receive the context and output a string. This is a common tactic for information that is always useful (for example, the user's name or the current date). Add it to the input when calling the Runner.run functions. This is similar to the instructions tactic, but allows you to have messages that are lower in the chain of command. Expose it via function tools. This is useful for on-demand context - the LLM decides when it needs some data, and can call the tool to fetch that data. Use retrieval or web search. These are special tools that are able to fetch relevant data from files or databases (retrieval), or from the web (web search). This is useful for "grounding" the response in relevant contextual data. Guardrails Guardrails run in parallel to your agents, enabling you to do checks and validations of user input. For example, imagine you have an agent that uses a very smart (and hence slow/expensive) model to help with customer requests. You wouldn't want malicious users to ask the model to help them with their math homework. So, you can run a guardrail with a fast/cheap model. If the guardrail detects malicious usage, it can immediately raise an error, which stops the expensive model from running and saves you time/money. There are two kinds of guardrails: Input guardrails run on the initial user input Output guardrails run on the final agent output Input guardrails Input guardrails run in 3 steps: First, the guardrail receives the same input passed to the agent. Next, the guardrail function runs to produce a GuardrailFunctionOutput, which is then wrapped in an InputGuardrailResult Finally, we check if .tripwire_triggered is true. If true, an InputGuardrailTripwireTriggered exception is raised, so you can appropriately respond to the user or handle the exception. Note Input guardrails are intended to run on user input, so an agent's guardrails only run if the agent is the first agent. You might wonder, why is the guardrails property on the agent instead of passed to Runner.run? It's because guardrails tend to be related to the actual Agent - you'd run different guardrails for different agents, so colocating the code is useful for readability. Output guardrails Output guardrails run in 3 steps: First, the guardrail receives the same input passed to the agent. Next, the guardrail function runs to produce a GuardrailFunctionOutput, which is then wrapped in an OutputGuardrailResult Finally, we check if .tripwire_triggered is true. If true, an OutputGuardrailTripwireTriggered exception is raised, so you can appropriately respond to the user or handle the exception. Note Output guardrails are intended to run on the final agent input, so an agent's guardrails only run if the agent is the last agent. Similar to the input guardrails, we do this because guardrails tend to be related to the actual Agent - you'd run different guardrails for different agents, so colocating the code is useful for readability. Tripwires If the input or output fails the guardrail, the Guardrail can signal this with a tripwire. As soon as we see a guardrail that has triggered the tripwires, we immediately raise a {Input,Output}GuardrailTripwireTriggered exception and halt the Agent execution. Implementing a guardrail You need to provide a function that receives input, and returns a GuardrailFunctionOutput. In this example, we'll do this by running an Agent under the hood. from pydantic import BaseModel from agents import ( Agent, GuardrailFunctionOutput, InputGuardrailTripwireTriggered, RunContextWrapper, Runner, TResponseInputItem, input_guardrail, ) class MathHomeworkOutput(BaseModel): is_math_homework: bool reasoning: str guardrail_agent = Agent( name="Guardrail check", instructions="Check if the user is asking you to do their math homework.", output_type=MathHomeworkOutput, ) @input_guardrail async def math_guardrail( ctx: RunContextWrapper[None], agent: Agent, input: str | list[TResponseInputItem] ) -> GuardrailFunctionOutput: result = await Runner.run(guardrail_agent, input, context=ctx.context) return GuardrailFunctionOutput( output_info=result.final_output, tripwire_triggered=result.final_output.is_math_homework, ) agent = Agent( name="Customer support agent", instructions="You are a customer support agent. You help customers with their questions.", input_guardrails=[math_guardrail], ) async def main(): # This should trip the guardrail try: await Runner.run(agent, "Hello, can you help me solve for x: 2x + 3 = 11?") print("Guardrail didn't trip - this is unexpected") except InputGuardrailTripwireTriggered: print("Math homework guardrail tripped") Output guardrails are similar. from pydantic import BaseModel from agents import ( Agent, GuardrailFunctionOutput, OutputGuardrailTripwireTriggered, RunContextWrapper, Runner, output_guardrail, ) class MessageOutput(BaseModel): response: str class MathOutput(BaseModel): is_math: bool reasoning: str guardrail_agent = Agent( name="Guardrail check", instructions="Check if the output includes any math.", output_type=MathOutput, ) @output_guardrail async def math_guardrail( ctx: RunContextWrapper, agent: Agent, output: MessageOutput ) -> GuardrailFunctionOutput: result = await Runner.run(guardrail_agent, output.response, context=ctx.context) return GuardrailFunctionOutput( output_info=result.final_output, tripwire_triggered=result.final_output.is_math, ) agent = Agent( name="Customer support agent", instructions="You are a customer support agent. You help customers with their questions.", output_guardrails=[math_guardrail], output_type=MessageOutput, ) async def main(): # This should trip the guardrail try: await Runner.run(agent, "Hello, can you help me solve for x: 2x + 3 = 11?") print("Guardrail didn't trip - this is unexpected") except OutputGuardrailTripwireTriggered: print("Math output guardrail tripped") Orchestrating multiple agents Orchestration refers to the flow of agents in your app. Which agents run, in what order, and how do they decide what happens next? There are two main ways to orchestrate agents: Allowing the LLM to make decisions: this uses the intelligence of an LLM to plan, reason, and decide on what steps to take based on that. Orchestrating via code: determining the flow of agents via your code. You can mix and match these patterns. Each has their own tradeoffs, described below. Orchestrating via LLM An agent is an LLM equipped with instructions, tools and handoffs. This means that given an open-ended task, the LLM can autonomously plan how it will tackle the task, using tools to take actions and acquire data, and using handoffs to delegate tasks to sub-agents. For example, a research agent could be equipped with tools like: Web search to find information online File search and retrieval to search through proprietary data and connections Computer use to take actions on a computer Code execution to do data analysis Handoffs to specialized agents that are great at planning, report writing and more. This pattern is great when the task is open-ended and you want to rely on the intelligence of an LLM. The most important tactics here are: Invest in good prompts. Make it clear what tools are available, how to use them, and what parameters it must operate within. Monitor your app and iterate on it. See where things go wrong, and iterate on your prompts. Allow the agent to introspect and improve. For example, run it in a loop, and let it critique itself; or, provide error messages and let it improve. Have specialized agents that excel in one task, rather than having a general purpose agent that is expected to be good at anything. Invest in evals. This lets you train your agents to improve and get better at tasks. Orchestrating via code While orchestrating via LLM is powerful, orchestrating via code makes tasks more deterministic and predictable, in terms of speed, cost and performance. Common patterns here are: Using structured outputs to generate well formed data that you can inspect with your code. For example, you might ask an agent to classify the task into a few categories, and then pick the next agent based on the category. Chaining multiple agents by transforming the output of one into the input of the next. You can decompose a task like writing a blog post into a series of steps - do research, write an outline, write the blog post, critique it, and then improve it. Running the agent that performs the task in a while loop with an agent that evaluates and provides feedback, until the evaluator says the output passes certain criteria. Running multiple agents in parallel, e.g. via Python primitives like asyncio.gather. This is useful for speed when you have multiple tasks that don't depend on each other. We have a number of examples in examples/agent_patterns. Models The Agents SDK comes with out-of-the-box support for OpenAI models in two flavors: Recommended: the OpenAIResponsesModel, which calls OpenAI APIs using the new Responses API. The OpenAIChatCompletionsModel, which calls OpenAI APIs using the Chat Completions API. Mixing and matching models Within a single workflow, you may want to use different models for each agent. For example, you could use a smaller, faster model for triage, while using a larger, more capable model for complex tasks. When configuring an Agent, you can select a specific model by either: Passing the name of an OpenAI model. Passing any model name + a ModelProvider that can map that name to a Model instance. Directly providing a Model implementation. Note While our SDK supports both the OpenAIResponsesModel and the OpenAIChatCompletionsModel shapes, we recommend using a single model shape for each workflow because the two shapes support a different set of features and tools. If your workflow requires mixing and matching model shapes, make sure that all the features you're using are available on both. from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel import asyncio spanish_agent = Agent( name="Spanish agent", instructions="You only speak Spanish.", model="o3-mini", ) english_agent = Agent( name="English agent", instructions="You only speak English", model=OpenAIChatCompletionsModel( model="gpt-4o", openai_client=AsyncOpenAI() ), ) triage_agent = Agent( name="Triage agent", instructions="Handoff to the appropriate agent based on the language of the request.", handoffs=[spanish_agent, english_agent], model="gpt-3.5-turbo", ) async def main(): result = await Runner.run(triage_agent, input="Hola, cmo ests?") print(result.final_output) Using other LLM providers Many providers also support the OpenAI API format, which means you can pass a base_url to the existing OpenAI model implementations and use them easily. ModelSettings is used to configure tuning parameters (e.g., temperature, top_p) for the model you select. external_client = AsyncOpenAI( api_key="EXTERNAL_API_KEY", base_url="https://api.external.com/v1/", ) spanish_agent = Agent( name="Spanish agent", instructions="You only speak Spanish.", model=OpenAIChatCompletionsModel( model="EXTERNAL_MODEL_NAME", openai_client=external_client, ), model_settings=ModelSettings(temperature=0.5), ) Configuring the SDK API keys and clients By default, the SDK looks for the OPENAI_API_KEY environment variable for LLM requests and tracing, as soon as it is imported. If you are unable to set that environment variable before your app starts, you can use the set_default_openai_key() function to set the key. from agents import set_default_openai_key set_default_openai_key("sk-...") Alternatively, you can also configure an OpenAI client to be used. By default, the SDK creates an AsyncOpenAI instance, using the API key from the environment variable or the default key set above. You can change this by using the set_default_openai_client() function. from openai import AsyncOpenAI from agents import set_default_openai_client custom_client = AsyncOpenAI(base_url="...", api_key="...") set_default_openai_client(custom_client) Finally, you can also customize the OpenAI API that is used. By default, we use the OpenAI Responses API. You can override this to use the Chat Completions API by using the set_default_openai_api() function. from agents import set_default_openai_api set_default_openai_api("chat_completions") Tracing Tracing is enabled by default. It uses the OpenAI API keys from the section above by default (i.e. the environment variable or the default key you set). You can specifically set the API key used for tracing by using the set_tracing_export_api_key function. from agents import set_tracing_export_api_key set_tracing_export_api_key("sk-...") You can also disable tracing entirely by using the set_tracing_disabled() function. from agents import set_tracing_disabled set_tracing_disabled(True) Debug logging The SDK has two Python loggers without any handlers set. By default, this means that warnings and errors are sent to stdout, but other logs are suppressed. To enable verbose logging, use the enable_verbose_stdout_logging() function. from agents import enable_verbose_stdout_logging enable_verbose_stdout_logging() Alternatively, you can customize the logs by adding handlers, filters, formatters, etc. You can read more in the Python logging guide. import logging logger = logging.getLogger("openai.agents") # or openai.agents.tracing for the Tracing logger # To make all logs show up logger.setLevel(logging.DEBUG) # To make info and above show up logger.setLevel(logging.INFO) # To make warning and above show up logger.setLevel(logging.WARNING) # etc # You can customize this as needed, but this will output to stderr by default logger.addHandler(logging.StreamHandler()) Sensitive data in logs Certain logs may contain sensitive data (for example, user data). If you want to disable this data from being logged, set the following environment variables. To disable logging LLM inputs and outputs: export OPENAI_AGENTS_DONT_LOG_MODEL_DATA=1 To disable logging tool inputs and outputs: export OPENAI_AGENTS_DONT_LOG_TOOL_DATA=1 Agents module set_default_openai_key set_default_openai_key(key: str) -> None Set the default OpenAI API key to use for LLM requests and tracing. This is only necessary if the OPENAI_API_KEY environment variable is not already set. If provided, this key will be used instead of the OPENAI_API_KEY environment variable. Source code in src/agents/__init__.py set_default_openai_client set_default_openai_client( client: AsyncOpenAI, use_for_tracing: bool = True ) -> None Set the default OpenAI client to use for LLM requests and/or tracing. If provided, this client will be used instead of the default OpenAI client. Parameters: Name Type Description Default client AsyncOpenAI The OpenAI client to use. required use_for_tracing bool Whether to use the API key from this client for uploading traces. If False, you'll either need to set the OPENAI_API_KEY environment variable or call set_tracing_export_api_key() with the API key you want to use for tracing. True Source code in src/agents/__init__.py set_default_openai_api set_default_openai_api( api: Literal["chat_completions", "responses"], ) -> None Set the default API to use for OpenAI LLM requests. By default, we will use the responses API but you can set this to use the chat completions API instead. Source code in src/agents/__init__.py set_tracing_export_api_key set_tracing_export_api_key(api_key: str) -> None Set the OpenAI API key for the backend exporter. Source code in src/agents/tracing/__init__.py set_tracing_disabled set_tracing_disabled(disabled: bool) -> None Set whether tracing is globally disabled. Source code in src/agents/tracing/__init__.py set_trace_processors set_trace_processors( processors: list[TracingProcessor], ) -> None Set the list of trace processors. This will replace the current list of processors. Source code in src/agents/tracing/__init__.py enable_verbose_stdout_logging enable_verbose_stdout_logging() Enables verbose logging to stdout. This is useful for debugging. Source code in src/agents/__init__.py Agents Agent dataclass Bases: Generic[TContext] An agent is an AI model configured with instructions, tools, guardrails, handoffs and more. We strongly recommend passing instructions, which is the "system prompt" for the agent. In addition, you can pass description, which is a human-readable description of the agent, used when the agent is used inside tools/handoffs. Agents are generic on the context type. The context is a (mutable) object you create. It is passed to tool functions, handoffs, guardrails, etc. Source code in src/agents/agent.py name instance-attribute name: str The name of the agent. instructions class-attribute instance-attribute instructions: ( str | Callable[ [RunContextWrapper[TContext], Agent[TContext]], MaybeAwaitable[str], ] | None ) = None The instructions for the agent. Will be used as the "system prompt" when this agent is invoked. Describes what the agent should do, and how it responds. Can either be a string, or a function that dynamically generates instructions for the agent. If you provide a function, it will be called with the context and the agent instance. It must return a string. handoff_description class-attribute instance-attribute handoff_description: str | None = None A description of the agent. This is used when the agent is used as a handoff, so that an LLM knows what it does and when to invoke it. handoffs class-attribute instance-attribute handoffs: list[Agent[Any] | Handoff[TContext]] = field( default_factory=list ) Handoffs are sub-agents that the agent can delegate to. You can provide a list of handoffs, and the agent can choose to delegate to them if relevant. Allows for separation of concerns and modularity. model class-attribute instance-attribute model: str | Model | None = None The model implementation to use when invoking the LLM. By default, if not set, the agent will use the default model configured in model_settings.DEFAULT_MODEL. model_settings class-attribute instance-attribute model_settings: ModelSettings = field( default_factory=ModelSettings ) Configures model-specific tuning parameters (e.g. temperature, top_p). tools class-attribute instance-attribute tools: list[Tool] = field(default_factory=list) A list of tools that the agent can use. input_guardrails class-attribute instance-attribute input_guardrails: list[InputGuardrail[TContext]] = field( default_factory=list ) A list of checks that run in parallel to the agent's execution, before generating a response. Runs only if the agent is the first agent in the chain. output_guardrails class-attribute instance-attribute output_guardrails: list[OutputGuardrail[TContext]] = field( default_factory=list ) A list of checks that run on the final output of the agent, after generating a response. Runs only if the agent produces a final output. output_type class-attribute instance-attribute output_type: type[Any] | None = None The type of the output object. If not provided, the output will be str. hooks class-attribute instance-attribute hooks: AgentHooks[TContext] | None = None A class that receives callbacks on various lifecycle events for this agent. clone clone(**kwargs: Any) -> Agent[TContext] Make a copy of the agent, with the given arguments changed. For example, you could do: new_agent = agent.clone(instructions="New instructions") Source code in src/agents/agent.py as_tool as_tool( tool_name: str | None, tool_description: str | None, custom_output_extractor: Callable[ [RunResult], Awaitable[str] ] | None = None, ) -> Tool Transform this agent into a tool, callable by other agents. This is different from handoffs in two ways: 1. In handoffs, the new agent receives the conversation history. In this tool, the new agent receives generated input. 2. In handoffs, the new agent takes over the conversation. In this tool, the new agent is called as a tool, and the conversation is continued by the original agent. Parameters: Name Type Description Default tool_name str | None The name of the tool. If not provided, the agent's name will be used. required tool_description str | None The description of the tool, which should indicate what it does and when to use it. required custom_output_extractor Callable[[RunResult], Awaitable[str]] | None A function that extracts the output from the agent. If not provided, the last message from the agent will be used. None Source code in src/agents/agent.py get_system_prompt async get_system_prompt( run_context: RunContextWrapper[TContext], ) -> str | None Get the system prompt for the agent. Source code in src/agents/agent.py Runner Runner Source code in src/agents/run.py run async classmethod run( starting_agent: Agent[TContext], input: str | list[TResponseInputItem], *, context: TContext | None = None, max_turns: int = DEFAULT_MAX_TURNS, hooks: RunHooks[TContext] | None = None, run_config: RunConfig | None = None, ) -> RunResult Run a workflow starting at the given agent. The agent will run in a loop until a final output is generated. The loop runs like so: 1. The agent is invoked with the given input. 2. If there is a final output (i.e. the agent produces something of type agent.output_type, the loop terminates. 3. If there's a handoff, we run the loop again, with the new agent. 4. Else, we run tool calls (if any), and re-run the loop. In two cases, the agent may raise an exception: 1. If the max_turns is exceeded, a MaxTurnsExceeded exception is raised. 2. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered exception is raised. Note that only the first agent's input guardrails are run. Parameters: Name Type Description Default starting_agent Agent[TContext] The starting agent to run. required input str | list[TResponseInputItem] The initial input to the agent. You can pass a single string for a user message, or a list of input items. required context TContext | None The context to run the agent with. None max_turns int The maximum number of turns to run the agent for. A turn is defined as one AI invocation (including any tool calls that might occur). DEFAULT_MAX_TURNS hooks RunHooks[TContext] | None An object that receives callbacks on various lifecycle events. None run_config RunConfig | None Global settings for the entire agent run. None Returns: Type Description RunResult A run result containing all the inputs, guardrail results and the output of the last RunResult agent. Agents may perform handoffs, so we don't know the specific type of the output. Source code in src/agents/run.py run_sync classmethod run_sync( starting_agent: Agent[TContext], input: str | list[TResponseInputItem], *, context: TContext | None = None, max_turns: int = DEFAULT_MAX_TURNS, hooks: RunHooks[TContext] | None = None, run_config: RunConfig | None = None, ) -> RunResult Run a workflow synchronously, starting at the given agent. Note that this just wraps the run method, so it will not work if there's already an event loop (e.g. inside an async function, or in a Jupyter notebook or async context like FastAPI). For those cases, use the run method instead. The agent will run in a loop until a final output is generated. The loop runs like so: 1. The agent is invoked with the given input. 2. If there is a final output (i.e. the agent produces something of type agent.output_type, the loop terminates. 3. If there's a handoff, we run the loop again, with the new agent. 4. Else, we run tool calls (if any), and re-run the loop. In two cases, the agent may raise an exception: 1. If the max_turns is exceeded, a MaxTurnsExceeded exception is raised. 2. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered exception is raised. Note that only the first agent's input guardrails are run. Parameters: Name Type Description Default starting_agent Agent[TContext] The starting agent to run. required input str | list[TResponseInputItem] The initial input to the agent. You can pass a single string for a user message, or a list of input items. required context TContext | None The context to run the agent with. None max_turns int The maximum number of turns to run the agent for. A turn is defined as one AI invocation (including any tool calls that might occur). DEFAULT_MAX_TURNS hooks RunHooks[TContext] | None An object that receives callbacks on various lifecycle events. None run_config RunConfig | None Global settings for the entire agent run. None Returns: Type Description RunResult A run result containing all the inputs, guardrail results and the output of the last RunResult agent. Agents may perform handoffs, so we don't know the specific type of the output. Source code in src/agents/run.py run_streamed classmethod run_streamed( starting_agent: Agent[TContext], input: str | list[TResponseInputItem], context: TContext | None = None, max_turns: int = DEFAULT_MAX_TURNS, hooks: RunHooks[TContext] | None = None, run_config: RunConfig | None = None, ) -> RunResultStreaming Run a workflow starting at the given agent in streaming mode. The returned result object contains a method you can use to stream semantic events as they are generated. The agent will run in a loop until a final output is generated. The loop runs like so: 1. The agent is invoked with the given input. 2. If there is a final output (i.e. the agent produces something of type agent.output_type, the loop terminates. 3. If there's a handoff, we run the loop again, with the new agent. 4. Else, we run tool calls (if any), and re-run the loop. In two cases, the agent may raise an exception: 1. If the max_turns is exceeded, a MaxTurnsExceeded exception is raised. 2. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered exception is raised. Note that only the first agent's input guardrails are run. Parameters: Name Type Description Default starting_agent Agent[TContext] The starting agent to run. required input str | list[TResponseInputItem] The initial input to the agent. You can pass a single string for a user message, or a list of input items. required context TContext | None The context to run the agent with. None max_turns int The maximum number of turns to run the agent for. A turn is defined as one AI invocation (including any tool calls that might occur). DEFAULT_MAX_TURNS hooks RunHooks[TContext] | None An object that receives callbacks on various lifecycle events. None run_config RunConfig | None Global settings for the entire agent run. None Returns: Type Description RunResultStreaming A result object that contains data about the run, as well as a method to stream events. Source code in src/agents/run.py RunConfig dataclass Configures settings for the entire agent run. Source code in src/agents/run.py model class-attribute instance-attribute model: str | Model | None = None The model to use for the entire agent run. If set, will override the model set on every agent. The model_provider passed in below must be able to resolve this model name. model_provider class-attribute instance-attribute model_provider: ModelProvider = field( default_factory=OpenAIProvider ) The model provider to use when looking up string model names. Defaults to OpenAI. model_settings class-attribute instance-attribute model_settings: ModelSettings | None = None Configure global model settings. Any non-null values will override the agent-specific model settings. handoff_input_filter class-attribute instance-attribute handoff_input_filter: HandoffInputFilter | None = None A global input filter to apply to all handoffs. If Handoff.input_filter is set, then that will take precedence. The input filter allows you to edit the inputs that are sent to the new agent. See the documentation in Handoff.input_filter for more details. input_guardrails class-attribute instance-attribute input_guardrails: list[InputGuardrail[Any]] | None = None A list of input guardrails to run on the initial run input. output_guardrails class-attribute instance-attribute output_guardrails: list[OutputGuardrail[Any]] | None = None A list of output guardrails to run on the final output of the run. tracing_disabled class-attribute instance-attribute tracing_disabled: bool = False Whether tracing is disabled for the agent run. If disabled, we will not trace the agent run. trace_include_sensitive_data class-attribute instance-attribute trace_include_sensitive_data: bool = True Whether we include potentially sensitive data (for example: inputs/outputs of tool calls or LLM generations) in traces. If False, we'll still create spans for these events, but the sensitive data will not be included. workflow_name class-attribute instance-attribute workflow_name: str = 'Agent workflow' The name of the run, used for tracing. Should be a logical name for the run, like "Code generation workflow" or "Customer support agent". trace_id class-attribute instance-attribute trace_id: str | None = None A custom trace ID to use for tracing. If not provided, we will generate a new trace ID. group_id class-attribute instance-attribute group_id: str | None = None A grouping identifier to use for tracing, to link multiple traces from the same conversation or process. For example, you might use a chat thread ID. trace_metadata class-attribute instance-attribute trace_metadata: dict[str, Any] | None = None An optional dictionary of additional metadata to include with the trace. Tools Tool module-attribute Tool = Union[ FunctionTool, FileSearchTool, WebSearchTool, ComputerTool, ] A tool that can be used in an agent. FunctionTool dataclass A tool that wraps a function. In most cases, you should use the function_tool helpers to create a FunctionTool, as they let you easily wrap a Python function. Source code in src/agents/tool.py name instance-attribute name: str The name of the tool, as shown to the LLM. Generally the name of the function. description instance-attribute description: str A description of the tool, as shown to the LLM. params_json_schema instance-attribute params_json_schema: dict[str, Any] The JSON schema for the tool's parameters. on_invoke_tool instance-attribute on_invoke_tool: Callable[ [RunContextWrapper[Any], str], Awaitable[str] ] A function that invokes the tool with the given context and parameters. The params passed are: 1. The tool run context. 2. The arguments from the LLM, as a JSON string. You must return a string representation of the tool output. In case of errors, you can either raise an Exception (which will cause the run to fail) or return a string error message (which will be sent back to the LLM). strict_json_schema class-attribute instance-attribute strict_json_schema: bool = True Whether the JSON schema is in strict mode. We strongly recommend setting this to True, as it increases the likelihood of correct JSON input. FileSearchTool dataclass A hosted tool that lets the LLM search through a vector store. Currently only supported with OpenAI models, using the Responses API. Source code in src/agents/tool.py vector_store_ids instance-attribute vector_store_ids: list[str] The IDs of the vector stores to search. max_num_results class-attribute instance-attribute max_num_results: int | None = None The maximum number of results to return. include_search_results class-attribute instance-attribute include_search_results: bool = False Whether to include the search results in the output produced by the LLM. ranking_options class-attribute instance-attribute ranking_options: RankingOptions | None = None Ranking options for search. filters class-attribute instance-attribute filters: Filters | None = None A filter to apply based on file attributes. WebSearchTool dataclass A hosted tool that lets the LLM search the web. Currently only supported with OpenAI models, using the Responses API. Source code in src/agents/tool.py user_location class-attribute instance-attribute user_location: UserLocation | None = None Optional location for the search. Lets you customize results to be relevant to a location. search_context_size class-attribute instance-attribute search_context_size: Literal["low", "medium", "high"] = ( "medium" ) The amount of context to use for the search. ComputerTool dataclass A hosted tool that lets the LLM control a computer. Source code in src/agents/tool.py computer instance-attribute computer: Computer | AsyncComputer The computer implementation, which describes the environment and dimensions of the computer, as well as implements the computer actions like click, screenshot, etc. default_tool_error_function default_tool_error_function( ctx: RunContextWrapper[Any], error: Exception ) -> str The default tool error function, which just returns a generic error message. Source code in src/agents/tool.py function_tool function_tool( func: ToolFunction[...], *, name_override: str | None = None, description_override: str | None = None, docstring_style: DocstringStyle | None = None, use_docstring_info: bool = True, failure_error_function: ToolErrorFunction | None = None, ) -> FunctionTool function_tool( *, name_override: str | None = None, description_override: str | None = None, docstring_style: DocstringStyle | None = None, use_docstring_info: bool = True, failure_error_function: ToolErrorFunction | None = None, ) -> Callable[[ToolFunction[...]], FunctionTool] function_tool( func: ToolFunction[...] | None = None, *, name_override: str | None = None, description_override: str | None = None, docstring_style: DocstringStyle | None = None, use_docstring_info: bool = True, failure_error_function: ToolErrorFunction | None = default_tool_error_function, ) -> ( FunctionTool | Callable[[ToolFunction[...]], FunctionTool] ) Decorator to create a FunctionTool from a function. By default, we will: 1. Parse the function signature to create a JSON schema for the tool's parameters. 2. Use the function's docstring to populate the tool's description. 3. Use the function's docstring to populate argument descriptions. The docstring style is detected automatically, but you can override it. If the function takes a RunContextWrapper as the first argument, it must match the context type of the agent that uses the tool. Parameters: Name Type Description Default func ToolFunction[...] | None The function to wrap. None name_override str | None If provided, use this name for the tool instead of the function's name. None description_override str | None If provided, use this description for the tool instead of the function's docstring. None docstring_style DocstringStyle | None If provided, use this style for the tool's docstring. If not provided, we will attempt to auto-detect the style. None use_docstring_info bool If True, use the function's docstring to populate the tool's description and argument descriptions. True failure_error_function ToolErrorFunction | None If provided, use this function to generate an error message when the tool call fails. The error message is sent to the LLM. If you pass None, then no error message will be sent and instead an Exception will be raised. default_tool_error_function Source code in src/agents/tool.py Results RunResultBase dataclass Bases: ABC Source code in src/agents/result.py input instance-attribute input: str | list[TResponseInputItem] The original input items i.e. the items before run() was called. This may be a mutated version of the input, if there are handoff input filters that mutate the input. new_items instance-attribute new_items: list[RunItem] The new items generated during the agent run. These include things like new messages, tool calls and their outputs, etc. raw_responses instance-attribute raw_responses: list[ModelResponse] The raw LLM responses generated by the model during the agent run. final_output instance-attribute final_output: Any The output of the last agent. input_guardrail_results instance-attribute input_guardrail_results: list[InputGuardrailResult] Guardrail results for the input messages. output_guardrail_results instance-attribute output_guardrail_results: list[OutputGuardrailResult] Guardrail results for the final output of the agent. last_agent abstractmethod property last_agent: Agent[Any] The last agent that was run. final_output_as final_output_as( cls: type[T], raise_if_incorrect_type: bool = False ) -> T A convenience method to cast the final output to a specific type. By default, the cast is only for the typechecker. If you set raise_if_incorrect_type to True, we'll raise a TypeError if the final output is not of the given type. Parameters: Name Type Description Default cls type[T] The type to cast the final output to. required raise_if_incorrect_type bool If True, we'll raise a TypeError if the final output is not of the given type. False Returns: Type Description T The final output casted to the given type. Source code in src/agents/result.py to_input_list to_input_list() -> list[TResponseInputItem] Creates a new input list, merging the original input with all the new items generated. Source code in src/agents/result.py RunResult dataclass Bases: RunResultBase Source code in src/agents/result.py last_agent property last_agent: Agent[Any] The last agent that was run. RunResultStreaming dataclass Bases: RunResultBase The result of an agent run in streaming mode. You can use the stream_events method to receive semantic events as they are generated. The streaming method will raise: - A MaxTurnsExceeded exception if the agent exceeds the max_turns limit. - A GuardrailTripwireTriggered exception if a guardrail is tripped. Source code in src/agents/result.py current_agent instance-attribute current_agent: Agent[Any] The current agent that is running. current_turn instance-attribute current_turn: int The current turn number. max_turns instance-attribute max_turns: int The maximum number of turns the agent can run for. final_output instance-attribute final_output: Any The final output of the agent. This is None until the agent has finished running. is_complete class-attribute instance-attribute is_complete: bool = False Whether the agent has finished running. last_agent property last_agent: Agent[Any] The last agent that was run. Updates as the agent run progresses, so the true last agent is only available after the agent run is complete. stream_events async stream_events() -> AsyncIterator[StreamEvent] Stream deltas for new items as they are generated. We're using the types from the OpenAI Responses API, so these are semantic events: each event has a type field that describes the type of the event, along with the data for that event. This will raise: - A MaxTurnsExceeded exception if the agent exceeds the max_turns limit. - A GuardrailTripwireTriggered exception if a guardrail is tripped. Source code in src/agents/result.py Streaming events StreamEvent module-attribute StreamEvent: TypeAlias = Union[ RawResponsesStreamEvent, RunItemStreamEvent, AgentUpdatedStreamEvent, ] A streaming event from an agent. RawResponsesStreamEvent dataclass Streaming event from the LLM. These are 'raw' events, i.e. they are directly passed through from the LLM. Source code in src/agents/stream_events.py data instance-attribute data: TResponseStreamEvent The raw responses streaming event from the LLM. type class-attribute instance-attribute type: Literal['raw_response_event'] = 'raw_response_event' The type of the event. RunItemStreamEvent dataclass Streaming events that wrap a RunItem. As the agent processes the LLM response, it will generate these events for new messages, tool calls, tool outputs, handoffs, etc. Source code in src/agents/stream_events.py name instance-attribute name: Literal[ "message_output_created", "handoff_requested", "handoff_occured", "tool_called", "tool_output", "reasoning_item_created", ] The name of the event. item instance-attribute item: RunItem The item that was created. AgentUpdatedStreamEvent dataclass Event that notifies that there is a new agent running. Source code in src/agents/stream_events.py new_agent instance-attribute new_agent: Agent[Any] The new agent. Handoffs HandoffInputFilter module-attribute HandoffInputFilter: TypeAlias = Callable[ [HandoffInputData], HandoffInputData ] A function that filters the input data passed to the next agent. HandoffInputData dataclass Source code in src/agents/handoffs.py input_history instance-attribute input_history: str | tuple[TResponseInputItem, ...] The input history before Runner.run() was called. pre_handoff_items instance-attribute pre_handoff_items: tuple[RunItem, ...] The items generated before the agent turn where the handoff was invoked. new_items instance-attribute new_items: tuple[RunItem, ...] The new items generated during the current agent turn, including the item that triggered the handoff and the tool output message representing the response from the handoff output. Handoff dataclass Bases: Generic[TContext] A handoff is when an agent delegates a task to another agent. For example, in a customer support scenario you might have a "triage agent" that determines which agent should handle the user's request, and sub-agents that specialize in different areas like billing, account management, etc. Source code in src/agents/handoffs.py tool_name instance-attribute tool_name: str The name of the tool that represents the handoff. tool_description instance-attribute tool_description: str The description of the tool that represents the handoff. input_json_schema instance-attribute input_json_schema: dict[str, Any] The JSON schema for the handoff input. Can be empty if the handoff does not take an input. on_invoke_handoff instance-attribute on_invoke_handoff: Callable[ [RunContextWrapper[Any], str], Awaitable[Agent[TContext]], ] The function that invokes the handoff. The parameters passed are: 1. The handoff run context 2. The arguments from the LLM, as a JSON string. Empty string if input_json_schema is empty. Must return an agent. agent_name instance-attribute agent_name: str The name of the agent that is being handed off to. input_filter class-attribute instance-attribute input_filter: HandoffInputFilter | None = None A function that filters the inputs that are passed to the next agent. By default, the new agent sees the entire conversation history. In some cases, you may want to filter inputs e.g. to remove older inputs, or remove tools from existing inputs. The function will receive the entire conversation history so far, including the input item that triggered the handoff and a tool call output item representing the handoff tool's output. You are free to modify the input history or new items as you see fit. The next agent that runs will receive handoff_input_data.all_items. IMPORTANT: in streaming mode, we will not stream anything as a result of this function. The items generated before will already have been streamed. strict_json_schema class-attribute instance-attribute strict_json_schema: bool = True Whether the input JSON schema is in strict mode. We strongly recommend setting this to True, as it increases the likelihood of correct JSON input. handoff handoff( agent: Agent[TContext], *, tool_name_override: str | None = None, tool_description_override: str | None = None, input_filter: Callable[ [HandoffInputData], HandoffInputData ] | None = None, ) -> Handoff[TContext] handoff( agent: Agent[TContext], *, on_handoff: OnHandoffWithInput[THandoffInput], input_type: type[THandoffInput], tool_description_override: str | None = None, tool_name_override: str | None = None, input_filter: Callable[ [HandoffInputData], HandoffInputData ] | None = None, ) -> Handoff[TContext] handoff( agent: Agent[TContext], *, on_handoff: OnHandoffWithoutInput, tool_description_override: str | None = None, tool_name_override: str | None = None, input_filter: Callable[ [HandoffInputData], HandoffInputData ] | None = None, ) -> Handoff[TContext] handoff( agent: Agent[TContext], tool_name_override: str | None = None, tool_description_override: str | None = None, on_handoff: OnHandoffWithInput[THandoffInput] | OnHandoffWithoutInput | None = None, input_type: type[THandoffInput] | None = None, input_filter: Callable[ [HandoffInputData], HandoffInputData ] | None = None, ) -> Handoff[TContext] Create a handoff from an agent. Parameters: Name Type Description Default agent Agent[TContext] The agent to handoff to, or a function that returns an agent. required tool_name_override str | None Optional override for the name of the tool that represents the handoff. None tool_description_override str | None Optional override for the description of the tool that represents the handoff. None on_handoff OnHandoffWithInput[THandoffInput] | OnHandoffWithoutInput | None A function that runs when the handoff is invoked. None input_type type[THandoffInput] | None the type of the input to the handoff. If provided, the input will be validated against this type. Only relevant if you pass a function that takes an input. None input_filter Callable[[HandoffInputData], HandoffInputData] | None a function that filters the inputs that are passed to the next agent. None Source code in src/agents/handoffs.py Lifecycle RunHooks Bases: Generic[TContext] A class that receives callbacks on various lifecycle events in an agent run. Subclass and override the methods you need. on_agent_start async on_agent_start( context: RunContextWrapper[TContext], agent: Agent[TContext], ) -> None Called before the agent is invoked. Called each time the current agent changes. on_agent_end async on_agent_end( context: RunContextWrapper[TContext], agent: Agent[TContext], output: Any, ) -> None Called when the agent produces a final output. on_handoff async on_handoff( context: RunContextWrapper[TContext], from_agent: Agent[TContext], to_agent: Agent[TContext], ) -> None Called when a handoff occurs. on_tool_start async on_tool_start( context: RunContextWrapper[TContext], agent: Agent[TContext], tool: Tool, ) -> None Called before a tool is invoked. on_tool_end async on_tool_end( context: RunContextWrapper[TContext], agent: Agent[TContext], tool: Tool, result: str, ) -> None Called after a tool is invoked. AgentHooks Bases: Generic[TContext] A class that receives callbacks on various lifecycle events for a specific agent. You can set this on agent.hooks to receive events for that specific agent. Subclass and override the methods you need. on_start async on_start( context: RunContextWrapper[TContext], agent: Agent[TContext], ) -> None Called before the agent is invoked. Called each time the running agent is changed to this agent. on_end async on_end( context: RunContextWrapper[TContext], agent: Agent[TContext], output: Any, ) -> None Called when the agent produces a final output. on_handoff async on_handoff( context: RunContextWrapper[TContext], agent: Agent[TContext], source: Agent[TContext], ) -> None Called when the agent is being handed off to. The source is the agent that is handing off to this agent. on_tool_start async on_tool_start( context: RunContextWrapper[TContext], agent: Agent[TContext], tool: Tool, ) -> None Called before a tool is invoked. on_tool_end async on_tool_end( context: RunContextWrapper[TContext], agent: Agent[TContext], tool: Tool, result: str, ) -> None Called after a tool is invoked. Items TResponse module-attribute TResponse = Response A type alias for the Response type from the OpenAI SDK. TResponseInputItem module-attribute TResponseInputItem = ResponseInputItemParam A type alias for the ResponseInputItemParam type from the OpenAI SDK. TResponseOutputItem module-attribute TResponseOutputItem = ResponseOutputItem A type alias for the ResponseOutputItem type from the OpenAI SDK. TResponseStreamEvent module-attribute TResponseStreamEvent = ResponseStreamEvent A type alias for the ResponseStreamEvent type from the OpenAI SDK. ToolCallItemTypes module-attribute ToolCallItemTypes: TypeAlias = Union[ ResponseFunctionToolCall, ResponseComputerToolCall, ResponseFileSearchToolCall, ResponseFunctionWebSearch, ] A type that represents a tool call item. RunItem module-attribute RunItem: TypeAlias = Union[ MessageOutputItem, HandoffCallItem, HandoffOutputItem, ToolCallItem, ToolCallOutputItem, ReasoningItem, ] An item generated by an agent. RunItemBase dataclass Bases: Generic[T], ABC Source code in src/agents/items.py agent instance-attribute agent: Agent[Any] The agent whose run caused this item to be generated. raw_item instance-attribute raw_item: T The raw Responses item from the run. This will always be a either an output item (i.e. openai.types.responses.ResponseOutputItem or an input item (i.e. openai.types.responses.ResponseInputItemParam). to_input_item to_input_item() -> TResponseInputItem Converts this item into an input item suitable for passing to the model. Source code in src/agents/items.py MessageOutputItem dataclass Bases: RunItemBase[ResponseOutputMessage] Represents a message from the LLM. Source code in src/agents/items.py raw_item instance-attribute raw_item: ResponseOutputMessage The raw response output message. HandoffCallItem dataclass Bases: RunItemBase[ResponseFunctionToolCall] Represents a tool call for a handoff from one agent to another. Source code in src/agents/items.py raw_item instance-attribute raw_item: ResponseFunctionToolCall The raw response function tool call that represents the handoff. HandoffOutputItem dataclass Bases: RunItemBase[TResponseInputItem] Represents the output of a handoff. Source code in src/agents/items.py raw_item instance-attribute raw_item: TResponseInputItem The raw input item that represents the handoff taking place. source_agent instance-attribute source_agent: Agent[Any] The agent that made the handoff. target_agent instance-attribute target_agent: Agent[Any] The agent that is being handed off to. ToolCallItem dataclass Bases: RunItemBase[ToolCallItemTypes] Represents a tool call e.g. a function call or computer action call. Source code in src/agents/items.py raw_item instance-attribute raw_item: ToolCallItemTypes The raw tool call item. ToolCallOutputItem dataclass Bases: RunItemBase[Union[FunctionCallOutput, ComputerCallOutput]] Represents the output of a tool call. Source code in src/agents/items.py raw_item instance-attribute raw_item: FunctionCallOutput | ComputerCallOutput The raw item from the model. output instance-attribute output: str The output of the tool call. ReasoningItem dataclass Bases: RunItemBase[ResponseReasoningItem] Represents a reasoning item. Source code in src/agents/items.py raw_item instance-attribute raw_item: ResponseReasoningItem The raw reasoning item. ModelResponse dataclass Source code in src/agents/items.py output instance-attribute output: list[TResponseOutputItem] A list of outputs (messages, tool calls, etc) generated by the model usage instance-attribute usage: Usage The usage information for the response. referenceable_id instance-attribute referenceable_id: str | None An ID for the response which can be used to refer to the response in subsequent calls to the model. Not supported by all model providers. to_input_items to_input_items() -> list[TResponseInputItem] Convert the output into a list of input items suitable for passing to the model. Source code in src/agents/items.py ItemHelpers Source code in src/agents/items.py extract_last_content classmethod extract_last_content(message: TResponseOutputItem) -> str Extracts the last text content or refusal from a message. Source code in src/agents/items.py extract_last_text classmethod extract_last_text( message: TResponseOutputItem, ) -> str | None Extracts the last text content from a message, if any. Ignores refusals. Source code in src/agents/items.py input_to_new_input_list classmethod input_to_new_input_list( input: str | list[TResponseInputItem], ) -> list[TResponseInputItem] Converts a string or list of input items into a list of input items. Source code in src/agents/items.py text_message_outputs classmethod text_message_outputs(items: list[RunItem]) -> str Concatenates all the text content from a list of message output items. Source code in src/agents/items.py text_message_output classmethod text_message_output(message: MessageOutputItem) -> str Extracts all the text content from a single message output item. Source code in src/agents/items.py tool_call_output_item classmethod tool_call_output_item( tool_call: ResponseFunctionToolCall, output: str ) -> FunctionCallOutput Creates a tool call output item from a tool call and its output. Source code in src/agents/items.py Run context RunContextWrapper dataclass Bases: Generic[TContext] This wraps the context object that you passed to Runner.run(). It also contains information about the usage of the agent run so far. NOTE: Contexts are not passed to the LLM. They're a way to pass dependencies and data to code you implement, like tool functions, callbacks, hooks, etc. Source code in src/agents/run_context.py context instance-attribute context: TContext The context object (or None), passed by you to Runner.run() usage class-attribute instance-attribute usage: Usage = field(default_factory=Usage) The usage of the agent run so far. For streamed responses, the usage will be stale until the last chunk of the stream is processed. Usage Usage dataclass Source code in src/agents/usage.py requests class-attribute instance-attribute requests: int = 0 Total requests made to the LLM API. input_tokens class-attribute instance-attribute input_tokens: int = 0 Total input tokens sent, across all requests. output_tokens class-attribute instance-attribute output_tokens: int = 0 Total output tokens received, across all requests. total_tokens class-attribute instance-attribute total_tokens: int = 0 Total tokens sent and received, across all requests. Exceptions AgentsException Bases: Exception Base class for all exceptions in the Agents SDK. Source code in src/agents/exceptions.py MaxTurnsExceeded Bases: AgentsException Exception raised when the maximum number of turns is exceeded. Source code in src/agents/exceptions.py ModelBehaviorError Bases: AgentsException Exception raised when the model does something unexpected, e.g. calling a tool that doesn't exist, or providing malformed JSON. Source code in src/agents/exceptions.py UserError Bases: AgentsException Exception raised when the user makes an error using the SDK. Source code in src/agents/exceptions.py InputGuardrailTripwireTriggered Bases: AgentsException Exception raised when a guardrail tripwire is triggered. Source code in src/agents/exceptions.py guardrail_result instance-attribute guardrail_result: InputGuardrailResult = guardrail_result The result data of the guardrail that was triggered. OutputGuardrailTripwireTriggered Bases: AgentsException Exception raised when a guardrail tripwire is triggered. Source code in src/agents/exceptions.py guardrail_result instance-attribute guardrail_result: OutputGuardrailResult = guardrail_result The result data of the guardrail that was triggered. Model settings ModelSettings dataclass Settings to use when calling an LLM. This class holds optional model configuration parameters (e.g. temperature, top_p, penalties, truncation, etc.). Source code in src/agents/model_settings.py resolve resolve(override: ModelSettings | None) -> ModelSettings Produce a new ModelSettings by overlaying any non-None values from the override on top of this instance. Source code in src/agents/model_settings.py Agent output AgentOutputSchema dataclass An object that captures the JSON schema of the output, as well as validating/parsing JSON produced by the LLM into the output type. Source code in src/agents/agent_output.py _type_adapter instance-attribute _type_adapter: TypeAdapter[Any] A type adapter that wraps the output type, so that we can validate JSON. _output_schema instance-attribute _output_schema: dict[str, Any] The JSON schema of the output. output_type instance-attribute output_type: type[Any] = output_type The type of the output. strict_json_schema instance-attribute strict_json_schema: bool = strict_json_schema Whether the JSON schema is in strict mode. We strongly recommend setting this to True, as it increases the likelihood of correct JSON input. _is_wrapped instance-attribute _is_wrapped: bool = not _is_subclass_of_base_model_or_dict( output_type ) Whether the output type is wrapped in a dictionary. This is generally done if the base output type cannot be represented as a JSON Schema object. __init__ __init__( output_type: type[Any], strict_json_schema: bool = True ) Parameters: Name Type Description Default output_type type[Any] The type of the output. required strict_json_schema bool Whether the JSON schema is in strict mode. We strongly recommend setting this to True, as it increases the likelihood of correct JSON input. True Source code in src/agents/agent_output.py is_plain_text is_plain_text() -> bool Whether the output type is plain text (versus a JSON object). Source code in src/agents/agent_output.py json_schema json_schema() -> dict[str, Any] The JSON schema of the output type. Source code in src/agents/agent_output.py validate_json validate_json(json_str: str, partial: bool = False) -> Any Validate a JSON string against the output type. Returns the validated object, or raises a ModelBehaviorError if the JSON is invalid. Source code in src/agents/agent_output.py output_type_name output_type_name() -> str The name of the output type. Source code in src/agents/agent_output.py Function schema FuncSchema dataclass Captures the schema for a python function, in preparation for sending it to an LLM as a tool. Source code in src/agents/function_schema.py name instance-attribute name: str The name of the function. description instance-attribute description: str | None The description of the function. params_pydantic_model instance-attribute params_pydantic_model: type[BaseModel] A Pydantic model that represents the function's parameters. params_json_schema instance-attribute params_json_schema: dict[str, Any] The JSON schema for the function's parameters, derived from the Pydantic model. signature instance-attribute signature: Signature The signature of the function. takes_context class-attribute instance-attribute takes_context: bool = False Whether the function takes a RunContextWrapper argument (must be the first argument). to_call_args to_call_args( data: BaseModel, ) -> tuple[list[Any], dict[str, Any]] Converts validated data from the Pydantic model into (args, kwargs), suitable for calling the original function. Source code in src/agents/function_schema.py FuncDocumentation dataclass Contains metadata about a python function, extracted from its docstring. Source code in src/agents/function_schema.py name instance-attribute name: str The name of the function, via __name__. description instance-attribute description: str | None The description of the function, derived from the docstring. param_descriptions instance-attribute param_descriptions: dict[str, str] | None The parameter descriptions of the function, derived from the docstring. generate_func_documentation generate_func_documentation( func: Callable[..., Any], style: DocstringStyle | None = None, ) -> FuncDocumentation Extracts metadata from a function docstring, in preparation for sending it to an LLM as a tool. Parameters: Name Type Description Default func Callable[..., Any] The function to extract documentation from. required style DocstringStyle | None The style of the docstring to use for parsing. If not provided, we will attempt to auto-detect the style. None Returns: Type Description FuncDocumentation A FuncDocumentation object containing the function's name, description, and parameter FuncDocumentation descriptions. Source code in src/agents/function_schema.py function_schema function_schema( func: Callable[..., Any], docstring_style: DocstringStyle | None = None, name_override: str | None = None, description_override: str | None = None, use_docstring_info: bool = True, strict_json_schema: bool = True, ) -> FuncSchema Given a python function, extracts a FuncSchema from it, capturing the name, description, parameter descriptions, and other metadata. Parameters: Name Type Description Default func Callable[..., Any] The function to extract the schema from. required docstring_style DocstringStyle | None The style of the docstring to use for parsing. If not provided, we will attempt to auto-detect the style. None name_override str | None If provided, use this name instead of the function's __name__. None description_override str | None If provided, use this description instead of the one derived from the docstring. None use_docstring_info bool If True, uses the docstring to generate the description and parameter descriptions. True strict_json_schema bool Whether the JSON schema is in strict mode. If True, we'll ensure that the schema adheres to the "strict" standard the OpenAI API expects. We strongly recommend setting this to True, as it increases the likelihood of the LLM providing correct JSON input. True Returns: Type Description FuncSchema A FuncSchema object containing the function's name, description, parameter descriptions, FuncSchema and other metadata. Model interface ModelTracing Bases: Enum Source code in src/agents/models/interface.py DISABLED class-attribute instance-attribute DISABLED = 0 Tracing is disabled entirely. ENABLED class-attribute instance-attribute ENABLED = 1 Tracing is enabled, and all data is included. ENABLED_WITHOUT_DATA class-attribute instance-attribute ENABLED_WITHOUT_DATA = 2 Tracing is enabled, but inputs/outputs are not included. Model Bases: ABC The base interface for calling an LLM. Source code in src/agents/models/interface.py get_response abstractmethod async get_response( system_instructions: str | None, input: str | list[TResponseInputItem], model_settings: ModelSettings, tools: list[Tool], output_schema: AgentOutputSchema | None, handoffs: list[Handoff], tracing: ModelTracing, ) -> ModelResponse Get a response from the model. Parameters: Name Type Description Default system_instructions str | None The system instructions to use. required input str | list[TResponseInputItem] The input items to the model, in OpenAI Responses format. required model_settings ModelSettings The model settings to use. required tools list[Tool] The tools available to the model. required output_schema AgentOutputSchema | None The output schema to use. required handoffs list[Handoff] The handoffs available to the model. required tracing ModelTracing Tracing configuration. required Returns: Type Description ModelResponse The full model response. Source code in src/agents/models/interface.py stream_response abstractmethod stream_response( system_instructions: str | None, input: str | list[TResponseInputItem], model_settings: ModelSettings, tools: list[Tool], output_schema: AgentOutputSchema | None, handoffs: list[Handoff], tracing: ModelTracing, ) -> AsyncIterator[TResponseStreamEvent] Stream a response from the model. Parameters: Name Type Description Default system_instructions str | None The system instructions to use. required input str | list[TResponseInputItem] The input items to the model, in OpenAI Responses format. required model_settings ModelSettings The model settings to use. required tools list[Tool] The tools available to the model. required output_schema AgentOutputSchema | None The output schema to use. required handoffs list[Handoff] The handoffs available to the model. required tracing ModelTracing Tracing configuration. required Returns: Type Description AsyncIterator[TResponseStreamEvent] An iterator of response stream events, in OpenAI Responses format. Source code in src/agents/models/interface.py ModelProvider Bases: ABC The base interface for a model provider. Model provider is responsible for looking up Models by name. Source code in src/agents/models/interface.py get_model abstractmethod get_model(model_name: str | None) -> Model Get a model by name. Parameters: Name Type Description Default model_name str | None The name of the model to get. required Returns: Type Description Model The model. OpenAI Responses model OpenAIResponsesModel Bases: Model Implementation of Model that uses the OpenAI Responses API. Source code in src/agents/models/openai_responses.py stream_response async stream_response( system_instructions: str | None, input: str | list[TResponseInputItem], model_settings: ModelSettings, tools: list[Tool], output_schema: AgentOutputSchema | None, handoffs: list[Handoff], tracing: ModelTracing, ) -> AsyncIterator[ResponseStreamEvent] Yields a partial message as it is generated, as well as the usage information. Source code in src/agents/models/openai_responses.py Converter Source code in src/agents/models/openai_responses.py _convert_tool classmethod _convert_tool( tool: Tool, ) -> tuple[ToolParam, IncludeLiteral | None] Returns converted tool and includes Source code in src/agents/models/openai_responses.py Tracing module TracingProcessor Bases: ABC Interface for processing spans. Source code in src/agents/tracing/processor_interface.py on_trace_start abstractmethod on_trace_start(trace: Trace) -> None Called when a trace is started. Parameters: Name Type Description Default trace Trace The trace that started. required Source code in src/agents/tracing/processor_interface.py on_trace_end abstractmethod on_trace_end(trace: Trace) -> None Called when a trace is finished. Parameters: Name Type Description Default trace Trace The trace that started. required Source code in src/agents/tracing/processor_interface.py on_span_start abstractmethod on_span_start(span: Span[Any]) -> None Called when a span is started. Parameters: Name Type Description Default span Span[Any] The span that started. required Source code in src/agents/tracing/processor_interface.py on_span_end abstractmethod on_span_end(span: Span[Any]) -> None Called when a span is finished. Should not block or raise exceptions. Parameters: Name Type Description Default span Span[Any] The span that finished. required Source code in src/agents/tracing/processor_interface.py shutdown abstractmethod shutdown() -> None Called when the application stops. Source code in src/agents/tracing/processor_interface.py force_flush abstractmethod force_flush() -> None Forces an immediate flush of all queued spans/traces. Source code in src/agents/tracing/processor_interface.py Span Bases: ABC, Generic[TSpanData] Source code in src/agents/tracing/spans.py start abstractmethod start(mark_as_current: bool = False) Start the span. Parameters: Name Type Description Default mark_as_current bool If true, the span will be marked as the current span. False Source code in src/agents/tracing/spans.py finish abstractmethod finish(reset_current: bool = False) -> None Finish the span. Parameters: Name Type Description Default reset_current bool If true, the span will be reset as the current span. False Source code in src/agents/tracing/spans.py Trace A trace is the root level object that tracing creates. It represents a logical "workflow". Source code in src/agents/tracing/traces.py trace_id abstractmethod property trace_id: str The trace ID. name abstractmethod property name: str The name of the workflow being traced. start abstractmethod start(mark_as_current: bool = False) Start the trace. Parameters: Name Type Description Default mark_as_current bool If true, the trace will be marked as the current trace. False Source code in src/agents/tracing/traces.py finish abstractmethod finish(reset_current: bool = False) Finish the trace. Parameters: Name Type Description Default reset_current bool If true, the trace will be reset as the current trace. False Source code in src/agents/tracing/traces.py export abstractmethod export() -> dict[str, Any] | None Export the trace as a dictionary. Source code in src/agents/tracing/traces.py agent_span agent_span( name: str, handoffs: list[str] | None = None, tools: list[str] | None = None, output_type: str | None = None, span_id: str | None = None, parent: Trace | Span[Any] | None = None, disabled: bool = False, ) -> Span[AgentSpanData] Create a new agent span. The span will not be started automatically, you should either do with agent_span() ... or call span.start() + span.finish() manually. Parameters: Name Type Description Default name str The name of the agent. required handoffs list[str] | None Optional list of agent names to which this agent could hand off control. None tools list[str] | None Optional list of tool names available to this agent. None output_type str | None Optional name of the output type produced by the agent. None span_id str | None The ID of the span. Optional. If not provided, we will generate an ID. We recommend using util.gen_span_id() to generate a span ID, to guarantee that IDs are correctly formatted. None parent Trace | Span[Any] | None The parent span or trace. If not provided, we will automatically use the current trace/span as the parent. None disabled bool If True, we will return a Span but the Span will not be recorded. False Returns: Type Description Span[AgentSpanData] The newly created agent span. Source code in src/agents/tracing/create.py custom_span custom_span( name: str, data: dict[str, Any] | None = None, span_id: str | None = None, parent: Trace | Span[Any] | None = None, disabled: bool = False, ) -> Span[CustomSpanData] Create a new custom span, to which you can add your own metadata. The span will not be started automatically, you should either do with custom_span() ... or call span.start() + span.finish() manually. Parameters: Name Type Description Default name str The name of the custom span. required data dict[str, Any] | None Arbitrary structured data to associate with the span. None span_id str | None The ID of the span. Optional. If not provided, we will generate an ID. We recommend using util.gen_span_id() to generate a span ID, to guarantee that IDs are correctly formatted. None parent Trace | Span[Any] | None The parent span or trace. If not provided, we will automatically use the current trace/span as the parent. None disabled bool If True, we will return a Span but the Span will not be recorded. False Returns: Type Description Span[CustomSpanData] The newly created custom span. Source code in src/agents/tracing/create.py function_span function_span( name: str, input: str | None = None, output: str | None = None, span_id: str | None = None, parent: Trace | Span[Any] | None = None, disabled: bool = False, ) -> Span[FunctionSpanData] Create a new function span. The span will not be started automatically, you should either do with function_span() ... or call span.start() + span.finish() manually. Parameters: Name Type Description Default name str The name of the function. required input str | None The input to the function. None output str | None The output of the function. None span_id str | None The ID of the span. Optional. If not provided, we will generate an ID. We recommend using util.gen_span_id() to generate a span ID, to guarantee that IDs are correctly formatted. None parent Trace | Span[Any] | None The parent span or trace. If not provided, we will automatically use the current trace/span as the parent. None disabled bool If True, we will return a Span but the Span will not be recorded. False Returns: Type Description Span[FunctionSpanData] The newly created function span. Source code in src/agents/tracing/create.py generation_span generation_span( input: Sequence[Mapping[str, Any]] | None = None, output: Sequence[Mapping[str, Any]] | None = None, model: str | None = None, model_config: Mapping[str, Any] | None = None, usage: dict[str, Any] | None = None, span_id: str | None = None, parent: Trace | Span[Any] | None = None, disabled: bool = False, ) -> Span[GenerationSpanData] Create a new generation span. The span will not be started automatically, you should either do with generation_span() ... or call span.start() + span.finish() manually. This span captures the details of a model generation, including the input message sequence, any generated outputs, the model name and configuration, and usage data. If you only need to capture a model response identifier, use response_span() instead. Parameters: Name Type Description Default input Sequence[Mapping[str, Any]] | None The sequence of input messages sent to the model. None output Sequence[Mapping[str, Any]] | None The sequence of output messages received from the model. None model str | None The model identifier used for the generation. None model_config Mapping[str, Any] | None The model configuration (hyperparameters) used. None usage dict[str, Any] | None A dictionary of usage information (input tokens, output tokens, etc.). None span_id str | None The ID of the span. Optional. If not provided, we will generate an ID. We recommend using util.gen_span_id() to generate a span ID, to guarantee that IDs are correctly formatted. None parent Trace | Span[Any] | None The parent span or trace. If not provided, we will automatically use the current trace/span as the parent. None disabled bool If True, we will return a Span but the Span will not be recorded. False Returns: Type Description Span[GenerationSpanData] The newly created generation span. Source code in src/agents/tracing/create.py get_current_span get_current_span() -> Span[Any] | None Returns the currently active span, if present. Source code in src/agents/tracing/create.py get_current_trace get_current_trace() -> Trace | None Returns the currently active trace, if present. Source code in src/agents/tracing/create.py guardrail_span guardrail_span( name: str, triggered: bool = False, span_id: str | None = None, parent: Trace | Span[Any] | None = None, disabled: bool = False, ) -> Span[GuardrailSpanData] Create a new guardrail span. The span will not be started automatically, you should either do with guardrail_span() ... or call span.start() + span.finish() manually. Parameters: Name Type Description Default name str The name of the guardrail. required triggered bool Whether the guardrail was triggered. False span_id str | None The ID of the span. Optional. If not provided, we will generate an ID. We recommend using util.gen_span_id() to generate a span ID, to guarantee that IDs are correctly formatted. None parent Trace | Span[Any] | None The parent span or trace. If not provided, we will automatically use the current trace/span as the parent. None disabled bool If True, we will return a Span but the Span will not be recorded. False Source code in src/agents/tracing/create.py handoff_span handoff_span( from_agent: str | None = None, to_agent: str | None = None, span_id: str | None = None, parent: Trace | Span[Any] | None = None, disabled: bool = False, ) -> Span[HandoffSpanData] Create a new handoff span. The span will not be started automatically, you should either do with handoff_span() ... or call span.start() + span.finish() manually. Parameters: Name Type Description Default from_agent str | None The name of the agent that is handing off. None to_agent str | None The name of the agent that is receiving the handoff. None span_id str | None The ID of the span. Optional. If not provided, we will generate an ID. We recommend using util.gen_span_id() to generate a span ID, to guarantee that IDs are correctly formatted. None parent Trace | Span[Any] | None The parent span or trace. If not provided, we will automatically use the current trace/span as the parent. None disabled bool If True, we will return a Span but the Span will not be recorded. False Returns: Type Description Span[HandoffSpanData] The newly created handoff span. Source code in src/agents/tracing/create.py response_span response_span( response: Response | None = None, span_id: str | None = None, parent: Trace | Span[Any] | None = None, disabled: bool = False, ) -> Span[ResponseSpanData] Create a new response span. The span will not be started automatically, you should either do with response_span() ... or call span.start() + span.finish() manually. Parameters: Name Type Description Default response Response | None The OpenAI Response object. None span_id str | None The ID of the span. Optional. If not provided, we will generate an ID. We recommend using util.gen_span_id() to generate a span ID, to guarantee that IDs are correctly formatted. None parent Trace | Span[Any] | None The parent span or trace. If not provided, we will automatically use the current trace/span as the parent. None disabled bool If True, we will return a Span but the Span will not be recorded. False Source code in src/agents/tracing/create.py trace trace( workflow_name: str, trace_id: str | None = None, group_id: str | None = None, metadata: dict[str, Any] | None = None, disabled: bool = False, ) -> Trace Create a new trace. The trace will not be started automatically; you should either use it as a context manager (with trace(...):) or call trace.start() + trace.finish() manually. In addition to the workflow name and optional grouping identifier, you can provide an arbitrary metadata dictionary to attach additional user-defined information to the trace. Parameters: Name Type Description Default workflow_name str The name of the logical app or workflow. For example, you might provide "code_bot" for a coding agent, or "customer_support_agent" for a customer support agent. required trace_id str | None The ID of the trace. Optional. If not provided, we will generate an ID. We recommend using util.gen_trace_id() to generate a trace ID, to guarantee that IDs are correctly formatted. None group_id str | None Optional grouping identifier to link multiple traces from the same conversation or process. For instance, you might use a chat thread ID. None metadata dict[str, Any] | None Optional dictionary of additional metadata to attach to the trace. None disabled bool If True, we will return a Trace but the Trace will not be recorded. This will not be checked if there's an existing trace and even_if_trace_running is True. False Returns: Type Description Trace The newly created trace object. Source code in src/agents/tracing/create.py gen_span_id gen_span_id() -> str Generates a new span ID. Source code in src/agents/tracing/util.py gen_trace_id gen_trace_id() -> str Generates a new trace ID. Source code in src/agents/tracing/util.py add_trace_processor add_trace_processor( span_processor: TracingProcessor, ) -> None Adds a new trace processor. This processor will receive all traces/spans. Source code in src/agents/tracing/__init__.py set_trace_processors set_trace_processors( processors: list[TracingProcessor], ) -> None Set the list of trace processors. This will replace the current list of processors. Source code in src/agents/tracing/__init__.py set_tracing_disabled set_tracing_disabled(disabled: bool) -> None Set whether tracing is globally disabled. Source code in src/agents/tracing/__init__.py set_tracing_export_api_key set_tracing_export_api_key(api_key: str) -> None Set the OpenAI API key for the backend exporter. Source code in src/agents/tracing/__init__.py Creating traces/spans trace trace( workflow_name: str, trace_id: str | None = None, group_id: str | None = None, metadata: dict[str, Any] | None = None, disabled: bool = False, ) -> Trace Create a new trace. The trace will not be started automatically; you should either use it as a context manager (with trace(...):) or call trace.start() + trace.finish() manually. In addition to the workflow name and optional grouping identifier, you can provide an arbitrary metadata dictionary to attach additional user-defined information to the trace. Parameters: Name Type Description Default workflow_name str The name of the logical app or workflow. For example, you might provide "code_bot" for a coding agent, or "customer_support_agent" for a customer support agent. required trace_id str | None The ID of the trace. Optional. If not provided, we will generate an ID. We recommend using util.gen_trace_id() to generate a trace ID, to guarantee that IDs are correctly formatted. None group_id str | None Optional grouping identifier to link multiple traces from the same conversation or process. For instance, you might use a chat thread ID. None metadata dict[str, Any] | None Optional dictionary of additional metadata to attach to the trace. None disabled bool If True, we will return a Trace but the Trace will not be recorded. This will not be checked if there's an existing trace and even_if_trace_running is True. False Returns: Type Description Trace The newly created trace object. Source code in src/agents/tracing/create.py get_current_trace get_current_trace() -> Trace | None Returns the currently active trace, if present. Source code in src/agents/tracing/create.py get_current_span get_current_span() -> Span[Any] | None Returns the currently active span, if present. Source code in src/agents/tracing/create.py agent_span agent_span( name: str, handoffs: list[str] | None = None, tools: list[str] | None = None, output_type: str | None = None, span_id: str | None = None, parent: Trace | Span[Any] | None = None, disabled: bool = False, ) -> Span[AgentSpanData] Create a new agent span. The span will not be started automatically, you should either do with agent_span() ... or call span.start() + span.finish() manually. Parameters: Name Type Description Default name str The name of the agent. required handoffs list[str] | None Optional list of agent names to which this agent could hand off control. None tools list[str] | None Optional list of tool names available to this agent. None output_type str | None Optional name of the output type produced by the agent. None span_id str | None The ID of the span. Optional. If not provided, we will generate an ID. We recommend using util.gen_span_id() to generate a span ID, to guarantee that IDs are correctly formatted. None parent Trace | Span[Any] | None The parent span or trace. If not provided, we will automatically use the current trace/span as the parent. None disabled bool If True, we will return a Span but the Span will not be recorded. False Returns: Type Description Span[AgentSpanData] The newly created agent span. Source code in src/agents/tracing/create.py function_span function_span( name: str, input: str | None = None, output: str | None = None, span_id: str | None = None, parent: Trace | Span[Any] | None = None, disabled: bool = False, ) -> Span[FunctionSpanData] Create a new function span. The span will not be started automatically, you should either do with function_span() ... or call span.start() + span.finish() manually. Parameters: Name Type Description Default name str The name of the function. required input str | None The input to the function. None output str | None The output of the function. None span_id str | None The ID of the span. Optional. If not provided, we will generate an ID. We recommend using util.gen_span_id() to generate a span ID, to guarantee that IDs are correctly formatted. None parent Trace | Span[Any] | None The parent span or trace. If not provided, we will automatically use the current trace/span as the parent. None disabled bool If True, we will return a Span but the Span will not be recorded. False Returns: Type Description Span[FunctionSpanData] The newly created function span. Source code in src/agents/tracing/create.py generation_span generation_span( input: Sequence[Mapping[str, Any]] | None = None, output: Sequence[Mapping[str, Any]] | None = None, model: str | None = None, model_config: Mapping[str, Any] | None = None, usage: dict[str, Any] | None = None, span_id: str | None = None, parent: Trace | Span[Any] | None = None, disabled: bool = False, ) -> Span[GenerationSpanData] Create a new generation span. The span will not be started automatically, you should either do with generation_span() ... or call span.start() + span.finish() manually. This span captures the details of a model generation, including the input message sequence, any generated outputs, the model name and configuration, and usage data. If you only need to capture a model response identifier, use response_span() instead. Parameters: Name Type Description Default input Sequence[Mapping[str, Any]] | None The sequence of input messages sent to the model. None output Sequence[Mapping[str, Any]] | None The sequence of output messages received from the model. None model str | None The model identifier used for the generation. None model_config Mapping[str, Any] | None The model configuration (hyperparameters) used. None usage dict[str, Any] | None A dictionary of usage information (input tokens, output tokens, etc.). None span_id str | None The ID of the span. Optional. If not provided, we will generate an ID. We recommend using util.gen_span_id() to generate a span ID, to guarantee that IDs are correctly formatted. None parent Trace | Span[Any] | None The parent span or trace. If not provided, we will automatically use the current trace/span as the parent. None disabled bool If True, we will return a Span but the Span will not be recorded. False Returns: Type Description Span[GenerationSpanData] The newly created generation span. Source code in src/agents/tracing/create.py response_span response_span( response: Response | None = None, span_id: str | None = None, parent: Trace | Span[Any] | None = None, disabled: bool = False, ) -> Span[ResponseSpanData] Create a new response span. The span will not be started automatically, you should either do with response_span() ... or call span.start() + span.finish() manually. Parameters: Name Type Description Default response Response | None The OpenAI Response object. None span_id str | None The ID of the span. Optional. If not provided, we will generate an ID. We recommend using util.gen_span_id() to generate a span ID, to guarantee that IDs are correctly formatted. None parent Trace | Span[Any] | None The parent span or trace. If not provided, we will automatically use the current trace/span as the parent. None disabled bool If True, we will return a Span but the Span will not be recorded. False Source code in src/agents/tracing/create.py handoff_span handoff_span( from_agent: str | None = None, to_agent: str | None = None, span_id: str | None = None, parent: Trace | Span[Any] | None = None, disabled: bool = False, ) -> Span[HandoffSpanData] Create a new handoff span. The span will not be started automatically, you should either do with handoff_span() ... or call span.start() + span.finish() manually. Parameters: Name Type Description Default from_agent str | None The name of the agent that is handing off. None to_agent str | None The name of the agent that is receiving the handoff. None span_id str | None The ID of the span. Optional. If not provided, we will generate an ID. We recommend using util.gen_span_id() to generate a span ID, to guarantee that IDs are correctly formatted. None parent Trace | Span[Any] | None The parent span or trace. If not provided, we will automatically use the current trace/span as the parent. None disabled bool If True, we will return a Span but the Span will not be recorded. False Returns: Type Description Span[HandoffSpanData] The newly created handoff span. Source code in src/agents/tracing/create.py custom_span custom_span( name: str, data: dict[str, Any] | None = None, span_id: str | None = None, parent: Trace | Span[Any] | None = None, disabled: bool = False, ) -> Span[CustomSpanData] Create a new custom span, to which you can add your own metadata. The span will not be started automatically, you should either do with custom_span() ... or call span.start() + span.finish() manually. Parameters: Name Type Description Default name str The name of the custom span. required data dict[str, Any] | None Arbitrary structured data to associate with the span. None span_id str | None The ID of the span. Optional. If not provided, we will generate an ID. We recommend using util.gen_span_id() to generate a span ID, to guarantee that IDs are correctly formatted. None parent Trace | Span[Any] | None The parent span or trace. If not provided, we will automatically use the current trace/span as the parent. None disabled bool If True, we will return a Span but the Span will not be recorded. False Returns: Type Description Span[CustomSpanData] The newly created custom span. Source code in src/agents/tracing/create.py guardrail_span guardrail_span( name: str, triggered: bool = False, span_id: str | None = None, parent: Trace | Span[Any] | None = None, disabled: bool = False, ) -> Span[GuardrailSpanData] Create a new guardrail span. The span will not be started automatically, you should either do with guardrail_span() ... or call span.start() + span.finish() manually. Parameters: Name Type Description Default name str The name of the guardrail. required triggered bool Whether the guardrail was triggered. False span_id str | None The ID of the span. Optional. If not provided, we will generate an ID. We recommend using util.gen_span_id() to generate a span ID, to guarantee that IDs are correctly formatted. None parent Trace | Span[Any] | None The parent span or trace. If not provided, we will automatically use the current trace/span as the parent. None disabled bool If True, we will return a Span but the Span will not be recorded. False Source code in src/agents/tracing/create.py Traces Trace A trace is the root level object that tracing creates. It represents a logical "workflow". Source code in src/agents/tracing/traces.py trace_id abstractmethod property trace_id: str The trace ID. name abstractmethod property name: str The name of the workflow being traced. start abstractmethod start(mark_as_current: bool = False) Start the trace. Parameters: Name Type Description Default mark_as_current bool If true, the trace will be marked as the current trace. False Source code in src/agents/tracing/traces.py finish abstractmethod finish(reset_current: bool = False) Finish the trace. Parameters: Name Type Description Default reset_current bool If true, the trace will be reset as the current trace. False Source code in src/agents/tracing/traces.py export abstractmethod export() -> dict[str, Any] | None Export the trace as a dictionary. Source code in src/agents/tracing/traces.py NoOpTrace Bases: Trace A no-op trace that will not be recorded. Source code in src/agents/tracing/traces.py TraceImpl Bases: Trace A trace that will be recorded by the tracing library. Source code in src/agents/tracing/traces.py Spans Span Bases: ABC, Generic[TSpanData] Source code in src/agents/tracing/spans.py start abstractmethod start(mark_as_current: bool = False) Start the span. Parameters: Name Type Description Default mark_as_current bool If true, the span will be marked as the current span. False Source code in src/agents/tracing/spans.py finish abstractmethod finish(reset_current: bool = False) -> None Finish the span. Parameters: Name Type Description Default reset_current bool If true, the span will be reset as the current span. False Source code in src/agents/tracing/spans.py NoOpSpan Bases: Span[TSpanData] Source code in src/agents/tracing/spans.py SpanImpl Bases: Span[TSpanData] Source code in src/agents/tracing/spans.py Processor interface TracingProcessor Bases: ABC Interface for processing spans. Source code in src/agents/tracing/processor_interface.py on_trace_start abstractmethod on_trace_start(trace: Trace) -> None Called when a trace is started. Parameters: Name Type Description Default trace Trace The trace that started. required Source code in src/agents/tracing/processor_interface.py on_trace_end abstractmethod on_trace_end(trace: Trace) -> None Called when a trace is finished. Parameters: Name Type Description Default trace Trace The trace that started. required Source code in src/agents/tracing/processor_interface.py on_span_start abstractmethod on_span_start(span: Span[Any]) -> None Called when a span is started. Parameters: Name Type Description Default span Span[Any] The span that started. required Source code in src/agents/tracing/processor_interface.py on_span_end abstractmethod on_span_end(span: Span[Any]) -> None Called when a span is finished. Should not block or raise exceptions. Parameters: Name Type Description Default span Span[Any] The span that finished. required Source code in src/agents/tracing/processor_interface.py shutdown abstractmethod shutdown() -> None Called when the application stops. Source code in src/agents/tracing/processor_interface.py force_flush abstractmethod force_flush() -> None Forces an immediate flush of all queued spans/traces. Source code in src/agents/tracing/processor_interface.py TracingExporter Bases: ABC Exports traces and spans. For example, could log them or send them to a backend. Source code in src/agents/tracing/processor_interface.py export abstractmethod export(items: list[Trace | Span[Any]]) -> None Exports a list of traces and spans. Parameters: Name Type Description Default items list[Trace | Span[Any]] The items to export. required Source code in src/agents/tracing/processor_interface.py Processors ConsoleSpanExporter Bases: TracingExporter Prints the traces and spans to the console. Source code in src/agents/tracing/processors.py BackendSpanExporter Bases: TracingExporter Source code in src/agents/tracing/processors.py __init__ __init__( api_key: str | None = None, organization: str | None = None, project: str | None = None, endpoint: str = "https://api.openai.com/v1/traces/ingest", max_retries: int = 3, base_delay: float = 1.0, max_delay: float = 30.0, ) Parameters: Name Type Description Default api_key str | None The API key for the "Authorization" header. Defaults to os.environ["OPENAI_TRACE_API_KEY"] if not provided. None organization str | None The OpenAI organization to use. Defaults to os.environ["OPENAI_ORG_ID"] if not provided. None project str | None The OpenAI project to use. Defaults to os.environ["OPENAI_PROJECT_ID"] if not provided. None endpoint str The HTTP endpoint to which traces/spans are posted. 'https://api.openai.com/v1/traces/ingest' max_retries int Maximum number of retries upon failures. 3 base_delay float Base delay (in seconds) for the first backoff. 1.0 max_delay float Maximum delay (in seconds) for backoff growth. 30.0 Source code in src/agents/tracing/processors.py set_api_key set_api_key(api_key: str) Set the OpenAI API key for the exporter. Parameters: Name Type Description Default api_key str The OpenAI API key to use. This is the same key used by the OpenAI Python client. required Source code in src/agents/tracing/processors.py close close() Close the underlying HTTP client. Source code in src/agents/tracing/processors.py BatchTraceProcessor Bases: TracingProcessor Some implementation notes: 1. Using Queue, which is thread-safe. 2. Using a background thread to export spans, to minimize any performance issues. 3. Spans are stored in memory until they are exported. Source code in src/agents/tracing/processors.py __init__ __init__( exporter: TracingExporter, max_queue_size: int = 8192, max_batch_size: int = 128, schedule_delay: float = 5.0, export_trigger_ratio: float = 0.7, ) Parameters: Name Type Description Default exporter TracingExporter The exporter to use. required max_queue_size int The maximum number of spans to store in the queue. After this, we will start dropping spans. 8192 max_batch_size int The maximum number of spans to export in a single batch. 128 schedule_delay float The delay between checks for new spans to export. 5.0 export_trigger_ratio float The ratio of the queue size at which we will trigger an export. 0.7 Source code in src/agents/tracing/processors.py shutdown shutdown(timeout: float | None = None) Called when the application stops. We signal our thread to stop, then join it. Source code in src/agents/tracing/processors.py force_flush force_flush() Forces an immediate flush of all queued spans. Source code in src/agents/tracing/processors.py _export_batches _export_batches(force: bool = False) Drains the queue and exports in batches. If force=True, export everything. Otherwise, export up to max_batch_size repeatedly until the queue is empty or below a certain threshold. Source code in src/agents/tracing/processors.py default_exporter default_exporter() -> BackendSpanExporter The default exporter, which exports traces and spans to the backend in batches. Source code in src/agents/tracing/processors.py default_processor default_processor() -> BatchTraceProcessor The default processor, which exports traces and spans to the backend in batches. Source code in src/agents/tracing/processors.py Setup SynchronousMultiTracingProcessor Bases: TracingProcessor Forwards all calls to a list of TracingProcessors, in order of registration. Source code in src/agents/tracing/setup.py add_tracing_processor add_tracing_processor(tracing_processor: TracingProcessor) Add a processor to the list of processors. Each processor will receive all traces/spans. Source code in src/agents/tracing/setup.py set_processors set_processors(processors: list[TracingProcessor]) Set the list of processors. This will replace the current list of processors. Source code in src/agents/tracing/setup.py on_trace_start on_trace_start(trace: Trace) -> None Called when a trace is started. Source code in src/agents/tracing/setup.py on_trace_end on_trace_end(trace: Trace) -> None Called when a trace is finished. Source code in src/agents/tracing/setup.py on_span_start on_span_start(span: Span[Any]) -> None Called when a span is started. Source code in src/agents/tracing/setup.py on_span_end on_span_end(span: Span[Any]) -> None Called when a span is finished. Source code in src/agents/tracing/setup.py shutdown shutdown() -> None Called when the application stops. Source code in src/agents/tracing/setup.py force_flush force_flush() Force the processors to flush their buffers. Source code in src/agents/tracing/setup.py TraceProvider Source code in src/agents/tracing/setup.py register_processor register_processor(processor: TracingProcessor) Add a processor to the list of processors. Each processor will receive all traces/spans. Source code in src/agents/tracing/setup.py set_processors set_processors(processors: list[TracingProcessor]) Set the list of processors. This will replace the current list of processors. Source code in src/agents/tracing/setup.py get_current_trace get_current_trace() -> Trace | None Returns the currently active trace, if any. Source code in src/agents/tracing/setup.py get_current_span get_current_span() -> Span[Any] | None Returns the currently active span, if any. Source code in src/agents/tracing/setup.py set_disabled set_disabled(disabled: bool) -> None Set whether tracing is disabled. Source code in src/agents/tracing/setup.py create_trace create_trace( name: str, trace_id: str | None = None, group_id: str | None = None, metadata: dict[str, Any] | None = None, disabled: bool = False, ) -> Trace Create a new trace. Source code in src/agents/tracing/setup.py create_span create_span( span_data: TSpanData, span_id: str | None = None, parent: Trace | Span[Any] | None = None, disabled: bool = False, ) -> Span[TSpanData] Create a new span. Source code in src/agents/tracing/setup.py Util time_iso time_iso() -> str Returns the current time in ISO 8601 format. Source code in src/agents/tracing/util.py gen_trace_id gen_trace_id() -> str Generates a new trace ID. Source code in src/agents/tracing/util.py gen_span_id gen_span_id() -> str Generates a new span ID. Source code in src/agents/tracing/util.py Handoff prompt RECOMMENDED_PROMPT_PREFIX module-attribute RECOMMENDED_PROMPT_PREFIX = "# System context\nYou are part of a multi-agent system called the Agents SDK, designed to make agent coordination and execution easy. Agents uses two primary abstraction: **Agents** and **Handoffs**. An agent encompasses instructions and tools and can hand off a conversation to another agent when appropriate. Handoffs are achieved by calling a handoff function, generally named transfer_to_<agent_name>. Transfers between agents are handled seamlessly in the background; do not mention or draw attention to these transfers in your conversation with the user.\n" prompt_with_handoff_instructions prompt_with_handoff_instructions(prompt: str) -> str Add recommended instructions to the prompt for agents that use handoffs. Source code in src/agents/extensions/handoff_prompt.py