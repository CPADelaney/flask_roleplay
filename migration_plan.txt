Axiom AI Framework: An Alternative Machine Learning Paradigm
Introduction
Axiom (Active eXpanding Inference with Object-centric Models) is a newly proposed AI framework that rethinks how machines learn, drawing inspiration from neuroscience and first principles rather than traditional deep learning. Developed by researchers at VERSES (with neuroscientist Karl Friston, originator of the free-energy principle, as a key contributor), Axiom is described as a “digital brain” architecture built to learn and adapt more like a human mind
verses.ai
wired.com
. In contrast to the ubiquitous artificial neural networks of today, Axiom employs active inference and built-in commonsense knowledge (axioms about the world) to achieve rapid learning with high data efficiency. Early results are striking – for example, Axiom was able to master several simple video games with about 97% less computation and 39× faster learning than a state-of-the-art deep neural network agent, while achieving ~60% better performance in gameplay
linkedin.com
. This suggests that Axiom’s approach could be a viable alternative paradigm for building intelligent agents, especially those that must learn efficiently and explainably from limited experience. In this report, we detail what the Axiom AI framework is and how it works, explain how it differs from traditional machine learning methods, and examine its technical architecture and unique algorithms. We then explore how Axiom might be integrated into a multi-agent AI system – including considerations for interfacing and communication between an Axiom-based agent and other agents. Finally, we discuss future directions and ideas to push this framework further, from potential improvements to novel applications and its role in the broader AI ecosystem.
What Is Axiom? Background and Conceptual Overview
Axiom is best understood as an alternative learning paradigm grounded in principles of cognitive science and Bayesian reasoning. It emerged from a desire to overcome key limitations of deep learning (such as data inefficiency, lack of transfer/generalization, and opaqueness) by instilling an AI agent with built-in knowledge and adaptive structure. Rather than starting with a blank-slate neural network that blindly optimizes millions of weights, Axiom begins with a set of core priors – innate assumptions about the environment – and a model structure that can self-organize and grow as needed
verses.ai
verses.ai
. These core priors serve as the AI’s initial “axioms” about how the world works, hence the name. For instance, in the context of video game experiments, Axiom was pre-equipped with commonsense notions like “the world consists of distinct objects,” “objects move continuously unless acted upon,” “objects can collide or interact when near each other,” etc
verses.ai
verses.ai
. Such baked-in knowledge is analogous to the intuitive physics and object permanence that human infants possess. It gives Axiom a head start in understanding dynamics, so it doesn’t need to relearn these basics from scratch via big data. At its heart, Axiom implements active inference, a theoretical framework from cognitive neuroscience. In active inference, an agent doesn’t just passively observe and learn correlations; instead, it maintains a generative model of the world and actively chooses actions that minimize the “surprise” (or prediction error) relative to that model
wired.com
. In simple terms, the agent is always asking: “If my internal model expects X to happen and something different happens, how can I change my model (learn) or take action to correct that discrepancy?” This means the agent is constantly learning and acting in tandem, seeking out informative experiences to refine its understanding
verses.ai
verses.ai
. Professor Friston notes that Axiom’s ability to “choose actions that enhance learning” – a principle missing from conventional RL – is a key to its sample-efficiency and fast adaptation
verses.ai
. Importantly, Axiom is not just a tweak to existing deep learning—it represents “a completely new approach to cognitive computing”, built from the ground up on first principles of perception and action
verses.ai
. VERSES’ team explicitly positions Axiom as “not a refinement of [the] current state-of-the-art” but a re-architecting of AI inspired by how real brains develop and learn
verses.ai
. The framework combines advances in neuroscience (Friston’s free-energy principle), reinforcement learning theory, and modern machine learning into a novel architecture termed a modular, biomimetic digital brain
verses.ai
. In practice, this means Axiom comprises distinct functional components (analogous to brain regions or modules) that handle perception, identity recognition, prediction, and so on, and these components interact via probabilistic message-passing (more on this below). By mirroring the brain’s organized, hierarchical structure, Axiom aims to achieve the kind of generalizable, transferrable intelligence that has so far eluded purely neural-network-based AI.
Technical Architecture and Methodology
Object-Centric Generative World Model: At the core of Axiom’s architecture is a probabilistic world model that is object-centric. Instead of representing the world as an unstructured collection of pixels or features, Axiom represents a scene as a set of discrete objects, each with interpretable properties (e.g. position, velocity, shape, color)
arxiv.org
. These objects and their interactions form the state variables of Axiom’s generative model. For example, in a game scenario, the model might maintain latent variables corresponding to the ball, the paddle, enemies, etc., and update beliefs about their states over time. Crucially, Axiom assumes a compositional structure of the world – that the state can be factorized into objects – which has been shown to aid generalization and robustness in other research
ar5iv.labs.arxiv.org
. This built-in bias aligns with human cognition (we perceive a scene as “things” and relations, not as raw pixels) and helps Axiom achieve higher sample efficiency and transfer learning
arxiv.org
. Each object in Axiom’s model has associated dynamic models and interaction models. In the initial implementation, object dynamics are modeled as piecewise-linear trajectories
arxiv.org
. In essence, an object is assumed to move with a certain linear motion (with some velocity) until an interaction (like a collision or control input) causes a change – a new piecewise segment. This is a simplifying assumption (many physical motions are roughly linear locally) that makes the model both expressive yet tractable. When two objects interact (come into contact, etc.), Axiom’s model accounts for a change in their trajectories or states in a sparse way (only when interactions happen)
arxiv.org
. This approach contrasts with a neural network that would try to implicitly approximate all possible motions through dense weights; Axiom explicitly encodes that “objects move in straight lines unless something causes a change,” which is a fair description of basic Newtonian physics in games. Modular Mixture Models & Expandable Structure: Axiom’s brain-like architecture is composed of multiple mixture models that handle different aspects of the inference process. The researchers describe components such as: a sensory mixture model (to extract object representations from raw sensory input), an identity mixture model (to classify objects into discrete types or categories), and a recurrent mixture model (to predict each object’s future trajectory and rewards)
ar5iv.labs.arxiv.org
ar5iv.labs.arxiv.org
. Each “mixture model” can be thought of as a set of candidate explanations or sub-models, where different components explain different patterns in the data. A key innovation is that Axiom’s generative model is not fixed: it can grow new components on the fly when it encounters observations that it cannot explain with its current set of models
arxiv.org
. For instance, if a novel object appears in the environment or a new type of event occurs (say an object “teleports” unexpectedly), Axiom will expand its model by adding a new component (e.g. a new object type or a new dynamic rule) to account for this phenomenon
arxiv.org
. This is analogous to a scientist adding a new hypothesis to explain an outlier, or a child inventing a new rule when they see something surprising. By expanding its structure as needed, Axiom avoids forcing new observations to fit a static model – a flexibility that traditional neural networks lack (a neural net has a fixed architecture and can only adjust weights, not add new neurons mid-stream). Of course, continually growing the model could lead to complexity bloat or overfitting, so Axiom also employs a compensatory mechanism: Bayesian model reduction. At intervals, the framework prunes or merges redundant components, simplifying the model without losing explanatory power
arxiv.org
. Essentially, Axiom can remove or generalize away parts of its structure that are not actually needed, distilling a more compact understanding after sufficient experience. This mirrors the way a brain might prune neural connections or how scientific theories are refined by eliminating unnecessary factors. The combination of expansion and reduction yields an evolving model that is both minimal and expressive, learning just the necessary complexity for the tasks at hand
arxiv.org
arxiv.org
. Variational Inference and Active Learning: Under the hood, Axiom performs Bayesian inference on its generative model to update its beliefs and make decisions. The design is implemented as a factor graph – a probabilistic graphical model – and the learning/inference is carried out via variational message passing on this graph
verses.ai
. In practical terms, instead of computing gradients to tweak weights (as in backpropagation), Axiom passes “messages” (summary statistics of probability distributions) between nodes in the factor graph to update posterior beliefs about unknown variables. This is a well-known technique in Bayesian networks and allows for distributed, parallelizable computation of beliefs. The benefit here is that Axiom’s learning is fast and online – it incorporates new observations in a single step of inference, adjusting its internal probability distributions without needing to replay millions of past examples or do lengthy gradient descent. In fact, Axiom’s creators emphasize that it learns “without the computational expense of gradient-based optimization”
arxiv.org
. The absence of backpropagation and large replay buffers is a huge departure from traditional deep RL, enabling Axiom to learn in minutes on a CPU/GPU what might take a neural net many hours on a powerful cluster
verses.ai
. Another hallmark of Axiom’s methodology is its active data acquisition. Because the agent’s action-selection is integrated into the learning loop (active inference paradigm), Axiom will intentionally perform actions that it expects will reduce uncertainty in its model
verses.ai
. For example, if the agent is unsure how a particular object behaves, it might poke or interact with that object to gather information – even if that action is not immediately geared toward maximizing reward. This curiosity-driven behavior is in contrast to standard reinforcement learning where the agent’s exploration is often random or solely reward-driven. By seeking out informative experiences, Axiom can learn the dynamics of the world more efficiently. Friston notes that this ability to “select data that enhances learning” is a key advancement over conventional approaches
verses.ai
. It means Axiom inherently balances exploration and exploitation in a principled way, focusing on actions that improve its world model. Summary of the Learning Loop: Putting it all together, Axiom’s operation cycle looks roughly like this: (1) Perception: from raw inputs (e.g. pixels), it infers an object-oriented representation (using its sensory mixture model to parse objects). (2) Prediction: using its current generative model, it predicts what should happen next and the expected outcomes of various actions. (3) Action: it chooses an action that either pursues its goal or reduces its prediction error (since those goals are formulated in terms of “expected free energy” or similar in active inference). (4) Observation & Update: after acting, it observes the actual outcome and compares it to the prediction. If something unexpected occurs (prediction error), it updates its beliefs via message passing. If the error is due to something fundamentally new (e.g. an unseen event), it expands its model structure to accommodate it. (5) Optionally, it might prune any parts of the model that are now superfluous (model reduction). This loop repeats continuously, allowing the agent to learn in real time as it interacts with the environment
wired.com
. Notably, learning and acting are intertwined – there isn’t a separate training phase with static data and then a deployment phase; the Axiom agent adapts on the fly.
Differences from Traditional Machine Learning Approaches
Axiom’s paradigm diverges from traditional machine learning and deep neural networks in several fundamental ways:
Built-in Knowledge and Priors vs. Tabula Rasa Learning: Traditional ML models typically start with minimal prior knowledge, aside from maybe some architectural biases. Deep neural nets, for instance, begin with random weights and must learn everything from large datasets, implicitly discovering any underlying rules. Axiom, by contrast, begins with explicit prior knowledge (axioms) about the domain. These core priors (e.g. object permanence, basic physics of motion) provide a scaffolding for learning
verses.ai
verses.ai
. This means Axiom doesn’t need to waste data to “rediscover” obvious truths – it already knows them and can focus on refining details. The inclusion of human-like priors allows for fast generalization to new tasks with minimal experience
arxiv.org
. Humans similarly leverage innate priors (like a notion of gravity or agency) to learn quickly; Axiom tries to do the same, whereas a standard deep RL agent might have to see thousands of trajectories before understanding object dynamics.
Dynamic, Self-Organizing Architecture vs. Fixed Architecture: Most ML models have a fixed structure determined before training (e.g. a neural net with N layers of M neurons each). If the task is harder or different than anticipated, the structure might be insufficient (or if the task is easier, the model may be larger than needed, but it won’t shrink by itself). Axiom’s framework is adaptive in structure – it can grow new model components when novel data demands it and remove components when they are no longer necessary
arxiv.org
. In essence, the model complexity is not fixed a priori; it self-assembles and optimizes its structure during learning
verses.ai
verses.ai
. This plasticity is analogous to neural plasticity in the brain, where neural connections can form or fade with experience. Traditional deep learning lacks such dynamic reconfiguration (aside from limited techniques like neural architecture search or pruning, which are not typically online or autonomous). Axiom’s approach means it can tackle a wider range of scenarios with the same framework, expanding for complexity and simplifying when possible. As Friston describes, Axiom’s core priors “endow [its] neural networks with the capacity to grow and learn their own structure, much like real brains”
verses.ai
.
Active Learning/Inference vs. Passive Training: Conventional supervised learning or reinforcement learning often treats data as given – the model is optimized on a fixed dataset or allowed to interact with an environment with a simple exploration policy (e.g. ε-greedy). In Axiom, learning is active and intrinsically motivated. The agent deliberately selects actions that will most improve its model (reduce uncertainty)
verses.ai
. This built-in curiosity or goal of information gain leads to far more efficient use of each interaction with the environment. Instead of needing millions of random trials to stumble upon the important experience, Axiom seeks out the experiences it expects will teach it the most. This is one reason why Axiom can reach competence in only 10k interaction steps on games where a deep RL like DreamerV3 needed 240k steps and still sometimes failed to learn the game
verses.ai
. In summary, Axiom flips the paradigm: rather than “learn after acting many times,” it learns while acting, continually, and chooses how to act in order to learn better. This active inference approach yields a marked improvement in sample efficiency, as empirically demonstrated (6× fewer steps needed to learn games, in one benchmark)
verses.ai
.
Bayesian, Explainable Reasoning vs. Opaque Black-Box Function Approximation: A major difference is interpretability. Axiom’s state consists of meaningful variables – objects with attributes, predicted trajectories, etc. – which are directly human-interpretable (you can inspect the model and see it thinks “object A is at position (x,y) moving right”)
verses.ai
. Its decision-making is based on probabilistic inferences that can be traced and even visualized (for instance, the research paper shows how Axiom’s model clusters trajectories associated with getting a reward vs a punishment, which provides a clear explanation of the agent’s strategy in terms of space and events
ar5iv.labs.arxiv.org
). Traditional deep learning models, in contrast, represent knowledge in thousands or millions of distributed weights and activations that do not correspond to clean semantic concepts. It’s famously difficult to understand why a neural network made a given decision, or what exactly it has learned. Axiom’s structured, factorized model offers transparent reasoning: every parameter has a meaning (e.g. the expected speed of the ball, or the probability of a collision outcome), and the inference process is essentially performing logical-belief updates that one can audit. This makes Axiom an explainable AI. Stakeholders can query its knowledge (e.g. “what do you believe will happen if I do X?”) and get an answer grounded in the model’s physics/logic. The framework’s designers highlight this contrast: Axiom’s object-centric variables “can be directly interpreted in human-readable terms… contrasting starkly against the opaque black-box reasoning of neural nets.”
verses.ai
 The improved explainability and predictability of Axiom’s behavior can foster trust in critical applications, where understanding AI decisions is as important as accuracy.
Efficiency (Data, Compute, and Energy): By leveraging priors and an information-seeking strategy, Axiom achieves results with far less data and computation than typical deep learning. In head-to-head tests (Gameworld 10k benchmark), Axiom reached higher performance in games with ~1/40th the GPU time and using a model that was ~440× smaller in parameter count than a leading deep RL agent
verses.ai
. Concretely, Axiom’s network had under 1 million parameters vs. ~420 million for the neural net (DreamerV3), and Axiom trained in about 10 minutes of GPU time compared to ~6.3 hours for the neural approach
verses.ai
. That equates to 97% less computational cost for Axiom to achieve superior or equal proficiency
linkedin.com
. This huge gain in efficiency means Axiom-like agents could potentially run on low-power devices at the edge (embedded systems, phones, IoT) rather than requiring cloud-scale compute. As one whitepaper noted, Axiom’s tiny footprint and energy savings make “high-performance intelligence practical on virtually any device,” enabling AI on “100 billion devices rather than from $100B data centers”
verses.ai
. In terms of data efficiency, the difference is equally stark: Axiom learned games with only a few thousand interaction steps, whereas neural nets often need hundreds of thousands or millions of steps/examples
verses.ai
. This suggests Axiom is well-suited to applications where data is scarce or expensive to obtain – a scenario where deep learning often struggles.
Online Lifelong Learning vs. Lengthy Offline Training: Traditional ML typically involves a distinct training phase (often offline, using historical data or simulated experience) and then a deployment phase where the model is fixed. If the environment changes or a new task comes up, one might have to collect new data and retrain or fine-tune the model, which is cumbersome. Axiom, by design, is a continual learning system. It learns during deployment, updating its knowledge on the fly. In the experiments, Axiom effectively “trained” and performed simultaneously, reaching good performance within minutes
wired.com
. There’s no need to stop and retrain from scratch when conditions change; the model simply adapts. Moreover, it doesn’t suffer from catastrophic forgetting in the same way, because its core priors and accumulated structure remain as it learns new scenarios (and it can recall or reuse them when relevant). This paradigm is closer to how human learning works (we don’t freeze our brain and retrain it entirely for a new task; we learn incrementally, building on what we know). For multi-task or evolving environments, Axiom’s approach could offer a significant advantage over static models.
In summary, Axiom distinguishes itself by being more brain-like: it starts with innate knowledge, learns by modeling and reasoning about the world (not just mapping inputs to outputs), actively explores to gather knowledge, and produces interpretable, causal models rather than inscrutable pattern recognition. The trade-off is that Axiom’s approach is more complex to design and may require more careful crafting of priors or model structure for each domain (whereas deep learning offers a more generic recipe of “take a big network and lots of data”). However, when applied appropriately, Axiom can yield AI systems that are more reliable, adaptable, efficient, and explainable – qualities highly desirable for reaching true general intelligence
verses.ai
verses.ai
. Researchers have lauded its originality; for instance, François Chollet (creator of the ARC challenge and a prominent AI scientist) commented that Axiom’s approach is “very original” and exactly the kind of new idea needed to push toward AGI, rather than staying on the beaten path of ever-bigger language models
wired.com
.
Integration into a Multi-Agent AI System
Integrating the Axiom framework into an existing multi-agent system requires careful consideration of interfacing, communication, and system design, due to Axiom’s distinctive operation. Here we discuss how an Axiom-based agent might be interfaced with other agents and what adaptations or protocols could facilitate its incorporation in a multi-agent architecture. Agent Interface and Environment Integration: At a basic level, an Axiom agent can be treated as an intelligent agent in the system with its own perception and action cycle. It will need a well-defined interface to the environment: this includes the sensor data it receives and the actions it can take. In a multi-agent simulation or real-world setup, one must provide Axiom with observations – for example, either raw input (like camera images) or a higher-level state (like a list of objects and their properties). Axiom is capable of parsing raw sensory inputs into objects on its own (using its sensory model), but if the environment or simulator can provide an object-level state directly, that could simplify integration. In practice, an adapter module might be written to convert the environment’s state representation into Axiom’s expected input format. Similarly, the Axiom agent will output actions (e.g. “move left” or “apply force to object X”) based on its active inference process. These actions need to be passed to the environment or a central simulator which then executes them. Many multi-agent platforms (for example, those using OpenAI Gym or ROS for robotics) could accommodate Axiom as an agent by implementing the standard callbacks: e.g. agent.observe(state), action = agent.act(), etc., where internally the Axiom agent updates its beliefs and chooses an action. One design adaptation to note is time management: Axiom performs continuous belief updates and may even benefit from a finer-grained time-step to accurately model dynamics. In a turn-based multi-agent loop, we might call Axiom’s update every tick with the latest observations. If multiple Axiom agents run concurrently, each will maintain its own world model (which could be from its perspective if observations differ). The system orchestrator should ensure that each agent’s model is synced with ground-truth as far as their observations permit. Unlike simpler agents that maybe react reflexively, Axiom agents maintain an internal state (their generative model) that carries over and accumulates knowledge – the system must allow them to persist this state between interactions (which is usually the case, but it means we shouldn’t reinitialize an Axiom agent between episodes unless desired). Communication Protocols for Multi-Agent Coordination: In a multi-agent system, agents often need to communicate or share knowledge. Axiom’s architecture is actually quite amenable to a principled communication scheme because it is built on a factor graph and probabilistic message passing. In fact, the creators of Axiom note that the same variational message-passing algorithms used within a single Axiom “brain” can extend to communication between different agents (they refer to “message passing and belief propagation… between synthetic (and real) brains”)
verses.ai
verses.ai
. This hints at a scenario where multiple Axiom agents could be connected in a larger factor graph, exchanging beliefs about shared variables. For example, if two Axiom agents are observing different parts of the same environment, each will have a probabilistic belief about the global state; by sending summarized beliefs (messages) to each other, they can converge on a consensus or at least a better informed overall state estimation (federated inference)
verses.ai
. Such an approach would be grounded in Bayesian theory – essentially performing a distributed inference where each agent only sees some data but the truth emerges from their combined information. The advantage of leveraging Axiom’s native message-passing is that communication can be quantitative and about uncertainties (not just symbolic), allowing agents to say things like “I’m 90% sure object A is at location (5,2)” to another agent, which can then incorporate that into its model. In practical terms, implementing this requires a communication protocol that can carry probabilistic model information. This could be realized through a custom protocol or by adopting emerging standards. Interestingly, VERSES and others have been involved in developing the IEEE P2874 “Spatial Web” standards, which aim to define protocols for sharing data and messages in distributed intelligent systems. Friston notes that new IEEE standards have been “carefully crafted to support the same kind of message passing and belief propagation” used by Axiom
verses.ai
. While the specifics of these standards are complex, the takeaway is that there is movement toward standardized agent communication that aligns with Axiom’s approach – meaning Axiom agents and non-Axiom agents could potentially communicate if everyone speaks this common protocol for exchanging state, context, goals, etc. For example, a standardized message might encode an agent’s belief about an object’s state or a request for information, in a way that any compliant agent (Axiom or otherwise) can parse. Using such interoperable protocols, an Axiom agent can join a multi-agent team and share what it “thinks” is happening, as well as update its model based on messages from others. If a formal agent communication language (like FIPA-ACL or a more modern alternative) is used in the multi-agent system, one might create performatives for exchanging model information or intentions. For instance, an Axiom agent could send a message like “(inform :content (state object1 position=… uncertainty=…))” to broadcast its belief about object1. Another agent receiving this could compare with its own view and either update its state or even reply if there’s a discrepancy (“I see object1 somewhere else – conflict!”). Designing these interactions may require some adaptation since Axiom’s internal state is rich; we likely wouldn’t dump the entire factor graph, but share key excerpts or predictions relevant to coordination. System Design Adaptations: To integrate Axiom, existing multi-agent system designs might need a few adjustments:
Common World Model or Translation Layer: If other agents in the system use a different world representation (say, a symbolic knowledge base or a simpler vector state), a translation mechanism is needed so that Axiom’s object-centric state can interface with that. One approach is to maintain a shared global state (a sort of blackboard or environment memory) that all agents read from and write to. The Axiom agent can update this global state with what it infers (for example, “object A moved here, object B is of type enemy”), and in turn consult it for information other agents have provided. Alternatively, each agent could maintain a private state and communicate changes, as discussed above. In either case, ensuring consistency between Axiom’s internal model and the overall system’s state is important. This may involve periodically syncing certain facts or resolving differences through negotiation or weighted averaging of beliefs.
Temporal Coordination: Multi-agent systems may be synchronous or asynchronous. Axiom’s continuous learning loop might run at its own pace, but when multiple agents are involved, a synchronization mechanism might be needed. For example, in a simulation, you might advance the clock step by step and let each agent (Axiom or otherwise) process and act. Axiom might sometimes choose to deliberate (update its model) a bit more if an observation was surprising. The system designer can decide whether to allow variable thinking times or enforce a step interval. If Axiom needs a bit more computation occasionally (e.g. when expanding or reducing its model), other agents might need to wait or operate in parallel if possible.
Multi-Agent Planning and Roles: In some systems, agents have specialized roles. An Axiom agent might be best suited as a model-building/forecasting agent given its strength in world modeling. For instance, consider a multi-agent autonomous driving system: some agents might be reactive controllers, another agent might be an Axiom-based modeler predicting the behavior of other cars/pedestrians. That Axiom agent could then feed its predictions to others. Designing the system to utilize Axiom’s strengths (prediction, understanding physics) while other agents handle complementary tasks (like communication with humans via natural language, or high-level goal selection) could yield a robust overall system. In such a design, the interface between the Axiom “world model” agent and the others becomes crucial – it might publish predictions or evaluations that others subscribe to.
Tool/Platform Integration: If the multi-agent system is built on a framework (e.g., a game engine, a robotics framework like ROS, or a multi-agent platform like JADE or SPADE), integrating Axiom might require writing a wrapper or plugin. The good news is the Axiom research has been released with reference code (the authors have released the Gameworld10k environment and Axiom code on GitHub
verses.ai
). One could use this codebase as a library: instantiate an Axiom agent object, feed it observations, and retrieve actions. Over time we can expect more API support as Axiom is productized (VERSES has announced integrating Axiom into its forthcoming Genius™ platform for autonomous agents
verses.ai
). The Genius platform presumably will make it easier to configure Axiom-based agents and connect them to external data streams or other agents. In a custom multi-agent setup, though, engineers should be prepared to handle some low-level integration details (e.g., converting image data to the format expected by Axiom’s vision model, ensuring numerical stability of the inference updates, etc.).
In essence, integrating Axiom into multi-agent systems involves treating it as an intelligent, model-based agent and ensuring there are pathways for it to exchange information with the rest of the system. With standardized message-passing protocols and shared world semantics, multiple Axiom agents could even directly cooperate, achieving what one might call multi-agent active inference. This opens the door to scenarios like distributed sensors/agents each running Axiom locally but pooling their understanding globally – achieving higher-level collective intelligence. Indeed, proponents suggest that with the proper standards and infrastructure, one could realize “intelligent transactions and federated inference” among Axiom-driven agents across a network (the so-called “spatial web” vision)
verses.ai
. While these ideas are forward-looking, they indicate that Axiom is inherently agent-oriented, and integrating it is not only feasible but potentially synergistic, given the right communication scaffolding.
Future Directions and Enhancements
Axiom is a fresh paradigm, and there are many avenues to extend and apply this framework within the broader AI ecosystem. Here we outline some potential improvements, applications, and ways to enhance Axiom’s role: 1. Scaling to Complex and Open-Ended Environments: So far, Axiom has proven itself on relatively simple arcade-style games with 2D physics. A clear next step is to scale it up to more complex environments – for example, 3D physics simulations, immersive video games, or even real-world robotic tasks. This will likely involve incorporating additional core priors (for 3D geometry, friction, kinematics, etc.) and ensuring the generative model can handle partial observability and noise (challenges that increase in the real world). It may also require optimizing the inference algorithms to handle a larger number of objects and more continuous state variables efficiently. The good news is that Axiom’s approach is modular and extensible by design; one can plug in new mixture model components for new modalities or dynamics. Future research might introduce hierarchical priors (e.g. an understanding of object groups or agents that have goals) to tackle scenarios like multi-agent interactions within the environment. Demonstrating Axiom in a robotics domain – say, a robot that actively learns how to manipulate objects it’s never seen before – would be a compelling validation and would push the framework to handle real-world complexity (sensing uncertainty, 3D collision dynamics, etc.). 2. Integration with Learning Perception Models: In the current implementation, Axiom’s perceptual front-end (extracting object representations from raw pixels) might rely on a predefined method (the paper hints at a Gaussian mixture model over pixel locations for segmentation
ar5iv.labs.arxiv.org
). Scaling to complex visuals (like high-resolution images or camera input in diverse environments) might benefit from incorporating learned perception modules – for example, using a convolutional neural network to propose object “slots” or features, which Axiom’s higher-level model can then take over. In other words, Axiom could be hybridized with deep learning for low-level perception while retaining its probabilistic object-centric core for high-level reasoning. This hybrid approach could combine the best of both worlds: neural networks for pattern recognition and Axiom’s structured model for reasoning and planning. Some ongoing research in object-centric deep learning (e.g. slot attention or AIR (Attend-Infer-Repeat) models) could complement Axiom’s approach
ar5iv.labs.arxiv.org
. By improving how Axiom interfaces with raw sensory data, we ensure it’s not limited by manually engineered perception in complex domains. 3. Knowledge and Language Integration: Axiom currently encodes “knowledge” in the form of mathematical priors about physics. Another frontier is integrating symbolic or semantic knowledge – for example, rules about the world that are not purely physical (like “if an object is an enemy, it will try to avoid your shots” or “red objects are harmful”). This could allow Axiom to operate in domains that involve logic, rules, or even human instructions. One exciting idea is to interface Axiom with a large language model (LLM) or knowledge graph. An LLM could provide high-level guidance or hypotheses (“perhaps this object works like a key for that door, based on description”) which Axiom could incorporate as priors to test. Conversely, Axiom could feed an LLM with grounded, up-to-date information from its world model to help the LLM reason about physical events. Such a synergy could yield agents that understand both physical causality (via Axiom) and abstract semantics (via an LLM). It would require careful alignment of representations – possibly translating Axiom’s object state into textual descriptions and vice versa – but recent work on multi-modal agents suggests this is a fruitful direction. By pushing in this direction, Axiom could become a component in broader AI systems where, for example, an agent has a “physics brain” (Axiom) and a “language brain” (LLM) working together. 4. Collaborative Multi-Agent Systems and Swarms: Building on the integration discussion, one could envision swarms or teams of Axiom-powered agents tackling complex tasks collaboratively. Each agent could specialize in part of the environment, and through shared active inference, the group could achieve a form of distributed cognition. For instance, imagine a fleet of autonomous drones using Axiom to model their local surroundings and share information to coordinate search-and-rescue operations. They could collectively maintain a map of an area, with each drone contributing its piece and Axiom’s message-passing ensuring consistency. This would push Axiom to handle not just single-agent learning, but also multi-agent learning where agents might even learn models of each other. Incorporating game theory or opponent modeling into the Axiom framework could enable agents that can adapt to other agents’ strategies – useful in competitive or cooperative environments. The active inference approach naturally extends to modeling other agents as part of the environment (treating another agent’s behavior as a stochastic process to predict), so this is a ripe area for research. It might require extending the generative model to include “intentions” or policies of others, essentially blending Axiom with ideas from interactive POMDPs or multi-agent RL. 5. Enhanced Planning and Goal Conditioning: In its current form, Axiom learns to handle environments and the implicit task (like game score maximization) by understanding the world dynamics. Future work can give Axiom more explicit goal-conditioning or planning capabilities on top of its world model. The arXiv paper already hints at a planning component (since being able to simulate outcomes is a strength of having a generative model)
ar5iv.labs.arxiv.org
. One could integrate algorithms like Monte Carlo Tree Search or goal-directed policy search that use Axiom’s model as a simulator to evaluate action sequences. Because Axiom’s model is probabilistic, a planner can query it: “if I do this, what is the predicted outcome distribution?” and plan accordingly. Enhancing the planning aspect could allow Axiom to tackle long-horizon problems and not just react myopically. Additionally, enabling goal swapping (where you can specify new objectives to the agent) would make it highly adaptable – essentially an AGI problem-solver that can be pointed at any goal within the realm of its world model. Some recent benchmarks like ARC (Abstraction and Reasoning Challenge) mentioned by Chollet emphasize general problem-solving ability
wired.com
; Axiom could be extended to such domains by incorporating symbolic goal conditions and using its inference machinery to satisfy them. 6. Formal Verification and Safety Enhancements: An intriguing area to push Axiom further is in verifiable AI. Because Axiom’s reasoning is transparent and grounded in explicit models, it may be possible to formally verify certain properties of its behavior (e.g. it will never violate a physical law or a safety rule encoded as a prior). We might add guardrail axioms – like “the agent must never go into region X of state space” – and Axiom would inherently respect those if they are part of its model. This is much harder to ensure in a black-box neural policy. A related effort, exemplified by another “Axiomatic AI” initiative in the industry, is to integrate formal logic/verification tools with AI for high-stakes domains
axiomatic-ai.com
axiomatic-ai.com
. Axiom is already aligned with this philosophy by design; future work could make it even more rigorous, for instance by proving that under its inference rules it will converge to optimal behavior or remain within safe bounds. Additionally, the alignment problem (ensuring AI’s goals align with human values) might be more tractable with Axiom because one can potentially encode human values as priors or constraints directly. An interesting research direction posits creating axioms for AI alignment – indeed some recent work has explored learning reward functions that satisfy axiomatic properties
arxiv.org
. Coupling such alignment techniques with Axiom’s framework could yield agents that not only learn efficiently, but are inherently aligned and auditably so. 7. Novel Applications in Science, Engineering, and Finance: Beyond traditional “AI tasks,” Axiom’s paradigm could unlock new applications. One area is scientific discovery and complex system modeling. Because Axiom can incorporate theoretical knowledge (via priors) and learn from data, it could serve as a lab assistant AI that helps researchers model phenomena quickly. For instance, in material science or semiconductor design, there are known physical laws but also experimental data – Axiom could take in both, adapt a model of a new material’s behavior, and suggest experiments (active inference naturally lends itself to suggesting informative experiments to perform). The startup Axiomatic AI (distinct from Axiom but philosophically related) is targeting photonic and semiconductor design with a formal+AI approach
axiomatic-ai.com
axiomatic-ai.com
, which shows the appetite for AI that understands domain theory. Similarly, in finance, as mentioned in the WIRED article, at least one financial company is experimenting with Axiom’s technology to model markets
wired.com
. Financial systems have some known rules (arbitrage principles, etc.) but also lots of stochastic data – an active inference agent could potentially learn market dynamics more efficiently than purely data-driven models, and even explain its predictions (useful for risk analysts). We might also see applications in healthcare (e.g. patient health modeling with physiological priors), operations research (adaptive supply chain management with priors about logistics), and any domain where combining prior knowledge with real-time learning is key. 8. Ecosystem and Tooling Development: To truly push Axiom’s role in the broader AI ecosystem, the community will need user-friendly tools, documentation, and integration with existing AI libraries. This includes developing high-level frameworks where developers can specify priors and domain structure in an accessible way (perhaps via a configuration or a modeling language) and then “compile” an Axiom agent for that domain. If Axiom can be packaged similarly to how PyTorch or TensorFlow models are, with an API for prediction and training (in this case, training is just the agent acting), it will encourage adoption. Open-sourcing the framework (with efficient implementations of the inference engine) is also important for academic and hobbyist uptake – the initial paper’s code release is a good start. Over time, one could imagine a library of common priors for different domains (like a plug-in module for 2D physics, one for 3D physics, one for social interactions, etc.) that people can use rather than reinventing from scratch each time. Moreover, integration with simulation environments (Unity, Unreal, Mujoco, etc.) and multi-agent platforms would make it easier to experiment with Axiom in various settings. If these developments occur, Axiom-like active inference agents could become a mainstream complement or alternative to neural networks in the AI toolbox. 9. Continuous Improvement of the Learning Algorithms: On the research side, pushing Axiom further will involve refining the underlying algorithms. This might mean improving the variational inference techniques for faster convergence, exploring different forms of the generative model (e.g. non-linear dynamics models instead of piecewise linear, if needed for certain domains), and enhancing stability. There may be scenarios where Axiom’s model expansion could overshoot (creating too many hypotheses) or its reduction might accidentally prune useful structure; tuning these processes or making them more robust (perhaps via hierarchical Bayesian priors that penalize overly complex models unless justified) will be important. Additionally, combining Axiom’s approach with deep learning might introduce challenges (like how to do end-to-end learning if part of the system is neural and part is probabilistic), so developing hybrid training methods would be useful. The reward handling in active inference (where rewards can be treated as observations of “surprise”) could also be an area of further research to ensure Axiom can handle a wide range of objective specifications, including sparse or delayed rewards. In conclusion, the Axiom AI framework represents a compelling shift towards AI with understanding – systems that know what they’re doing in a way that’s accessible to us and that can learn new things rapidly. Its technical approach of marrying prior knowledge with on-line learning and probabilistic planning is a refreshing contrast to the brute-force scaling of deep learning. As we have discussed, integrating such a framework into larger AI systems will require thoughtful design, but also offers big payoffs in terms of robustness and cooperation. Moving forward, expanding Axiom’s capabilities and applying it to real-world problems will test its mettle and likely inspire further innovations. If successful, Axiom and similar “axiomatic” approaches could significantly enhance the AI ecosystem, enabling more trustworthy, efficient, and generally intelligent agents that complement or even surpass today’s neural networks
reddit.com
verses.ai
. The journey has just begun, but Axiom’s early achievements in mastering games in minutes hint that we may be witnessing the rise of a new paradigm – one that brings us a step closer to AI that thinks and learns as broadly and efficiently as we do.
References and Sources
Heins, C. et al. (2025). “AXIOM: Learning to Play Games in Minutes with Expanding Object-Centric Models.” arXiv preprint arXiv:2505.24784
arxiv.org
arxiv.org
. (Technical paper introducing the Axiom framework, detailing its architecture and performance on the Gameworld benchmark.)
VERSES AI – Whitepaper & Blog on AXIOM “Digital Brain” (June 2025)
verses.ai
verses.ai
. (Overview of Axiom’s design philosophy, core priors, and results, authored by the company behind Axiom. Explains differences from deep learning and real-world implications.)
VERSES AI – Newsletter (June 2025), Karl Friston’s comments
verses.ai
verses.ai
. (Insights from Friston on how Axiom grows its structure and uses active inference, and notes on integration with standards and the Genius platform.)
Will Knight, WIRED Magazine (June 11, 2025). “A Deep Learning Alternative Can Help AI Agents Gameplay the Real World.”
wired.com
wired.com
. (Popular press article describing Axiom in lay terms, including quotes from Friston and others on its significance, and examples of its efficiency and potential use cases like finance.)
Denise Holt, LinkedIn Post (June 2025) summarizing Diginomica article
linkedin.com
. (Mentions Axiom’s performance vs. a perceptron-based approach – 60% better scores, 97% more efficient, 39× faster – on a benchmark, highlighting its advantages over traditional neural networks.)
VERSES AI – Open Letter/Press Release (Dec 19, 2023)
reddit.com
reddit.com
. (Announces VERSES’ active inference approach to AGI and invites collaboration. Describes active inference agents as using orders of magnitude less data and compute than deep learning, with explainable decision-making – context for Axiom’s goals.)
Axiomatic AI (company website, 2025)
axiomatic-ai.com
axiomatic-ai.com
. (Background on a related effort combining formal verification with AI for scientific applications, illustrating the broader trend of “axiomatic” techniques to improve reliability and interpretability in AI systems.)

Integrating Axiom’s Object-Centric Active Inference into Nyx’s AI Architecture
Current Nyx Architecture Overview
NyxBrain as Central “Brain”: In the Nyx system (found in nyx/core), the NyxBrain class (defined in nyx/core/brain/base.py) serves as the central agent coordinating various AI subsystems. It is essentially the “ultimate agent” or brain of Nyx, responsible for processing input, invoking specialized modules (memory, emotion, etc.), and generating outputs. For example, NyxBrain’s process_input pipeline currently orchestrates steps like emotion analysis, memory recall, and response generation by delegating to sub-agents or tools. In the current design, NyxBrain is implemented with mixins for extended functionality (e.g. logging, distributed context) and uses asynchronous methods to handle input and output flows
GitHub
GitHub
. It holds references to subsystem components (memory, emotional core, etc.) and uses them during processing. Modular Subsystems (Memory, Emotion, etc.): Nyx’s AI is composed of specialized modules, each handling a piece of cognition:
Memory System: The memory subsystem (see nyx/core/memory_core.py and memory_orchestrator.py) manages the agent’s memories of observations, reflections, and experiences. It stores memory entries (text with metadata) and provides retrieval, creation, and maintenance operations. Currently, memories are primarily text-based, tagged with metadata such as significance, emotional context, and associated entities
GitHub
. The memory system uses a orchestrator agent to delegate tasks to specialist sub-agents for retrieval, reflection, maintenance, etc.
GitHub
GitHub
. This means that even memory operations often involve LLM-driven agents (e.g. a Memory Orchestrator agent that parses requests and hands off to a Retrieval Specialist or Reflection Specialist
GitHub
). The memory storage is in-memory with indices by type, tag, scope, etc., but entities are just stored as strings and indexed for lookup
GitHub
 – there isn’t yet a first-class object representation of entities in the world.
Emotional and Theory-of-Mind Modeling: Nyx includes an emotional state module and a rudimentary Theory of Mind. For instance, nyx/core/theory_of_mind.py maintains a UserMentalState model per user, tracking inferred emotions, goals, beliefs, etc.
GitHub
GitHub
. NyxBrain’s processing pipeline will use an emotional_core (if present) to analyze sentiment of input and update the agent’s emotional state
GitHub
. If emotional_core is not available, it falls back to a default (e.g. neutral state)
GitHub
GitHub
. This indicates some separation of concerns: emotional analysis can be done by a deterministic module (for efficiency) or by an agent prompt.
Perception & Response Generation: Nyx processes user inputs by feeding them to the main LLM-based agent(s) along with tools. In the AgentProcessor (nyx/core/brain/processing/agent.py), the system defines multiple agent personas: e.g. a Main Agent (Nyx’s primary personality) and supporting ones like a Creative Agent for storytelling and an Analytical Agent
GitHub
GitHub
. The main agent prompt includes instructions to consider emotional context and past experiences
GitHub
, and it has function tools attached for analyzing emotion, retrieving memories, and storing new experiences
GitHub
. This effectively serves as the “perception + cognition” stage – the user’s raw text plus context is handed to an LLM that calls tools to interpret emotion or fetch relevant memories. After processing, NyxBrain then synthesizes a response (potentially via another agent or a final composition step).
Goal/Planning System: Nyx has a goal management system (nyx/core/goal_system.py) that supports longer-term objectives, planning steps, and conflict resolution. Goals are represented with structured models and can consist of sequences of steps (each step corresponds to an action method on NyxBrain, like “query_knowledge” or “generate_response”)
GitHub
GitHub
. This indicates NyxBrain exposes certain actions that the planning system can call. The planning/goal system ensures multi-step plans are executed and can leverage memory and other modules to decide next steps.
Cross-Module Coordination: Recent iterations of Nyx introduced a Context Distribution (Agent-to-Agent, or A2A) system and a Global Workspace mechanism to improve module cooperation. The context distribution (nyx/core/brain/context_distribution.py along with integration_layer.py) establishes a shared blackboard (SharedContext) where modules publish updates and subscribe to relevant context
GitHub
GitHub
. For example, as input is processed, each module (memory, emotion, goals, etc.) can contribute to the shared context (writing its analysis results, e.g. emotional_state, memory_context) and can react to context updates from others
GitHub
GitHub
. This is essentially a message-passing framework within Nyx. The Global Workspace (in nyx/core/global_workspace/) expands on this by defining a NyxEngine that runs a cognitive loop combining modules like Memory, Emotion, Reasoning, etc., within a global broadcasting architecture. A snippet from the Global Workspace integration guide shows how NyxBrain can be wired to a NyxEngine composed of such modules
GitHub
GitHub
. In that example, NyxBrain.initialize() instantiates modules (MemoryModule, EmotionModule, etc.) and starts the global workspace engine, and then NyxBrain.process_input() simply delegates to NyxEngine.process_input()
GitHub
GitHub
.
In summary, Nyx’s current design is modular and event-driven. On each user input, NyxBrain gathers context (emotional state, recent memories, etc.), calls on specialized sub-agents or tools to interpret the input and retrieve knowledge, then generates a response (possibly coordinating multiple agent outputs). It uses a hub-and-spoke model (NyxBrain as the hub calling modules), though recent additions (context distribution, global workspace) are moving it toward a more coordinated network of modules. However, the knowledge and state are still largely handled in an episodic, textual form (memories, chat history) rather than a persistent world model of objects. NyxBrain’s loop is mostly reactive to user input – there isn’t a continuous background simulation of the world, aside from periodic tasks like memory decay or goal monitoring.
Axiom’s Paradigm: Object-Centric Active Inference
Object-Centric World Modeling: The Axiom AI framework (as described) emphasizes representing the agent’s knowledge as a structured world of objects – each object could be an entity (NPC, player, item, location, concept) with its own state, attributes, and relations. Instead of treating memories purely as text entries with tags, an object-centric approach would maintain an internal world model (like a knowledge graph or environment state) where those entities and their properties are first-class citizens. For example, the user and NPC characters would be objects with attributes (emotional state, goals, etc.), and events would update these object properties. Even abstract concepts like “current scene” or “weather” might be objects in the world model. This is a shift from Nyx’s current memory system: rather than just indexing text by entity names, the agent would store information in structured object records (possibly with relationships between objects). Nyx does have hints of object-centric thinking – e.g., memory metadata tracks an entities list and the Theory-of-Mind keeps a dictionary of UserMentalState objects for each user
GitHub
GitHub
 – but these are not fully integrated as a single world representation. In Axiom’s paradigm, you would likely see a unified state representation where, for instance, the user’s mental state, their inventory of items, the NPC’s last known location, etc., are all part of an object graph that the AI can query and update. Active Inference & Continuous Loop: Active inference (from cognitive science) involves an agent continuously predicting its sensory inputs and choosing actions to minimize the “surprise” (prediction error) relative to its internal model. In practical terms, an active inference agent doesn’t just react after the fact; it actively anticipates and plans to fulfill its beliefs or goals, adjusting its internal model when observations differ from predictions. Integrating this into Nyx means shifting from a purely turn-based reactive loop to a more continuous perception–prediction–action cycle:
The agent would maintain beliefs about the world (in the object-centric model).
Upon new input (or even over time without input), it updates its beliefs (Bayesian belief update or a simpler state update) using the difference between expected and observed data.
It then selects an action (which could be speaking a line of dialogue, initiating a narrative event, asking the user a question, etc.) that it expects will move the world toward a desired state or will reduce uncertainty in its model.
This cycle repeats continuously or at discrete timesteps. In an interactive fiction context, this might mean Nyx could occasionally take initiative or at least internally simulate the next likely user action as part of planning its responses.
In Axiom’s paradigm, each cognitive module (perception, memory, planning) operates within this predictive loop. Perception isn’t just passing the raw user message to an LLM; it could involve parsing the input into state changes (e.g. update object properties: “user moved to location X” or “user is sad now”). Action selection would leverage a generative model of outcomes – Nyx might internally simulate “if I tell the user this story or perform this action, what do I expect to happen next?” and choose actions that best align with its goals or minimize surprise. This is more sophisticated than Nyx’s current “generate a plausible response via LLM” – it implies an element of planning and validation of actions against an internal world model. Modularity and Message-Passing: Axiom likely uses a modular architecture as well (common in cognitive frameworks), but modules communicate via the state of the world or via message passing rather than direct calls. Nyx’s context distribution system already lays groundwork for this: modules broadcast state updates (e.g. memory module shares retrieved facts, emotion module shares updated emotions) and subscribe to relevant changes
GitHub
GitHub
. An Axiom-like design would formalize this into perhaps an event bus or blackboard where object state changes are the primary currency of communication. For example, instead of NyxBrain explicitly calling memory_orchestrator.retrieve_memories(...), the perception module could post a “query” event, and a memory module listening for that event provides results which get written into the shared context (or directly into the world model). Nyx has already moved toward an event-driven flow with SharedContext updates
GitHub
GitHub
, though currently the context is partitioned by module (emotional_state, memory_context, etc.) rather than by real-world entities. Aligning with Axiom might involve refactoring this context to revolve around objects (for instance, a “world state” object that multiple modules update different fields of, or a collection of objects each module can access). Summary of Gaps: In short, Nyx’s design is partially aligned with Axiom’s principles, but not fully:
Object-centric: Nyx has some object-like data structures (user profiles, indexes of entities in memory) but no unified object store or true knowledge graph of the world. State is scattered (memories, user mental state, goal lists, etc.). Moving to Axiom means consolidating these into an integrated world model representation.
Active inference & continuous loop: Nyx mostly reacts to inputs and doesn’t explicitly simulate ahead or act unprompted. There is no persistent loop constantly running inside NyxBrain aside from asynchronous tasks (like memory decay). Adopting active inference would likely require NyxBrain (or a new Axiom engine within it) to maintain a persistent loop that iteratively updates beliefs and considers actions – even if those actions are only executed when relevant (e.g., only speak when the user is expecting a response).
Modularity & messaging: Nyx is already modular and is introducing message-passing (context distribution), which is good alignment. However, those messages need to carry structured state info about objects for a truly object-centric approach. Currently, for example, the memory module returns a list of memory dicts as JSON to the main agent
GitHub
 rather than updating a shared structured knowledge store that other modules can directly reason over. An Axiom integration would push Nyx to share state in a more direct data form (like updating the properties of a character object) rather than solely through the main agent’s prompt context.
Required Changes for Axiom Integration
To integrate Axiom’s object-centric, active-inference paradigm into Nyx, several adaptations and restructuring steps are necessary:
1. World Model and Memory Restructuring
Introduce a Structured World State: Nyx will need a persistent World Model component that holds objects representing the key entities and environmental state. In code, this could be a new class or set of classes (e.g. WorldState with collections of Entity objects, or an ObjectStore). Each Entity might have attributes and methods to update its state. For example, you might define classes for Character (with properties like mood, knowledge, location), Location, Item, etc., depending on the domain of the roleplay. This world model becomes the source of truth for the agent’s understanding of the environment, in contrast to the current free-form memory. Map Existing Memory into the World Model: The information in Nyx’s memory system needs to be folded into this object-centric model. Currently, Nyx’s memory entries carry an entities list to tag which actors or concepts are involved
GitHub
. These can be used to connect memory to objects: e.g., a memory “Saw a dragon in the cave” has entities ["dragon", "cave"]. In an object-centric design, you would have a Dragon object and a Cave object in the world model, and that memory could be attached to those objects (perhaps as part of their history or as a link in a knowledge graph). Practically, one might extend the Memory model to include actual references to Entity objects (instead of just names), or maintain a dictionary mapping entity names to object instances for lookup. A preparatory step is to ensure each important entity has a unique identifier (Nyx might already use names or IDs; e.g., user_id for the user, NPC names for characters). Using those IDs, you can populate the world model initially from the current knowledge: for each unique entity in memory, instantiate an object; for each memory fact, update object attributes or link it to those objects. Revamp Memory Storage/Query: With an object store in place, retrieving information should rely less on LLM “query understanding” and more on direct data queries. For example, instead of using the MemoryRetriever agent to interpret a query and search
GitHub
GitHub
, you can implement a method in the world model like find_memories(entity=X, related_to=Y) or get_recent_events(location=Z), etc., using efficient lookups (the current MemoryStorage indexes by entity, tags, etc., which can be repurposed in an object context
GitHub
GitHub
). This would significantly increase efficiency and determinism. The Memory Orchestrator agent (which uses GPT to decide which specialist to hand off to
GitHub
) could be deprecated or simplified, since in an Axiom approach the logic of “which memory operation to do” can be handled by coded policies or the active inference mechanism. For example, rather than prompting an LLM to decide between retrieving or reflecting, the agent’s active inference strategy might inherently know when to retrieve facts (e.g., when it has high uncertainty about something in its world model, it queries memory) vs. when to create a reflection (when multiple facts need summarizing into an insight). Ensure Consistency Between Memory and World State: In a hybrid phase, you might run both systems in parallel – keep writing new events both to the legacy memory store and to the new world model – to avoid losing functionality while transitioning. Eventually, the goal would be to have the world model subsume much of the memory’s role: episodic memories could be stored as part of objects (like a character’s “experience log”), and semantic knowledge as object attributes or relations. Maintaining backward compatibility might involve writing adapter methods (for instance, have brain.memory_orchestrator.retrieve_memories() call into the new world model query functions internally).
2. Perception Pipeline Changes
Dedicated Perception Module: Introduce an explicit perception step where raw inputs (user messages or environmental changes) are parsed into updates to the world model. Right now, perception is intertwined with the LLM’s job – e.g., the main agent prompt is responsible for understanding the user input and perhaps deducing some emotion or facts from it. With an Axiom-style integration, you might create a Perception Module that runs before the main cognition. This module would:
Extract factual content from the input (e.g., convert “I travel to the forest” into an update like player.location = forest in the world model).
Detect contextual signals such as the user’s emotion or intent (for instance, parse sentiment or key phrases to update the user’s mood or goal in their object). Nyx already has a mechanism for sentiment analysis using brain.emotional_core
GitHub
 – this can be folded into the perception module so that after parsing the text, it calls emotional_core.analyze_text_sentiment and updates the user’s emotional state object.
Update the SharedContext (if using context distribution) with these findings so other modules see the new info. For example, after perception, a context update could broadcast “User mood: sad, Location: forest changed” which the memory or narrative modules could pick up.
Implementing perception could be done with either rule-based NLP, a smaller language model prompt, or a combination (for complex text, a prompt to parse into a structured representation could help). The key is that the output of perception is not directly a memory entry string, but a structured update to the internal state. In code, you might create nyx/core/brain/perception_module.py or integrate it into the existing processing pipeline. The Global Workspace “starter kit” hints at modules like an ExpressionModule or ReasoningModule
GitHub
 – similarly, you’d have a Parsing/Perception Module. If following the global workspace pattern, you would instantiate this module and add it to the NyxEngine modules list in NyxBrain initialization
GitHub
. Sensor Data and Object Creation: If new objects appear in input (say the user mentions a new character or item that wasn’t known), the perception step should be able to create new object instances in the world model (or flag it for later confirmation). This dynamic object management will make the system truly object-centric in how it accumulates knowledge over time (as opposed to just appending to memory lists).
3. Action Selection & Active Inference Loop
Implement an Active Inference Loop: Rather than only running cognition upon user inputs, NyxBrain (or the new Axiom-based engine within it) should maintain a loop that continually evaluates the state and decides if action is needed. This doesn’t mean Nyx will spam the user unprompted; rather, internally it can simulate or plan several steps ahead. For example, after processing input and updating the world model, the engine could:
Predict what the user might do or feel next given the current state (this could use the Theory-of-Mind model – which under active inference becomes a generative model of the user’s state).
Formulate possible responses or actions and simulate their effects on the world model. For instance, the AI might consider two possible next messages to the user and internally “imagine” how the user (object) would react or how the story world changes, using its generative model. This is analogous to mental simulation, a concept quite aligned with active inference planning.
Choose the action that best reduces uncertainty or furthers the agent’s goals. Goals in Nyx (from goal_system.py) already have structures like priorities, relationships, and can call NyxBrain’s methods
GitHub
. Under active inference, these goals would be tied to the agent’s prior preferences (desirable states it tries to achieve). You may need to adjust how goals are evaluated: for instance, incorporate an “expected free energy” calculation (a measure combining achieving desired outcomes and gaining information). While a full Bayesian implementation is complex, a heuristic version can be built on top of the existing goal planner – e.g., mark certain goals as epistemic (information-gathering) vs pragmatic (achieving an outcome) and ensure the planner sometimes selects actions to gather info when uncertainty is high.
In code terms, establishing this loop could mean:
Refactoring NyxBrain’s process_input and response generation logic to not immediately finalize a response. Instead, after updating state from the input, call into a planner/active-inference module (could be part of the Axiom engine) that may iterate through a few perception → update → propose action cycles internally. Only then produce a final response. This might require asynchronous looping with timeouts or a fixed number of iterations per user turn to avoid stalls.
The global workspace NyxEngine already runs a start() which could maintain a loop (the example shows await self.engine.start()
GitHub
). If Axiom provides a similar engine, you would hook it similarly. For instance, if there’s an AxiomEngine class that encapsulates an active inference process, you would initialize it in NyxBrain.initialize() and possibly run it in the background (e.g., asyncio.create_task(self.axiom_engine.run())). Then NyxBrain.process_input() would feed new evidence to this engine and perhaps wait for the engine to output a decided action (the dialogue response).
If no explicit Axiom library is provided, you can emulate active inference: e.g., implement a loop in NyxBrain that after each user input continues to “tick” through a few internal planning steps (using the existing goal system and world model).
Action Representation: Responses or other agent actions should be represented as action objects (or at least structured data), not just text. Nyx partially has this with the concept of a ProcessResult and ResponseResult models
GitHub
GitHub
 and in goals, each step has an action field (which corresponds to a NyxBrain method)
GitHub
. In an Axiom integration, you would formalize actions in the world model – for example, define an Action class or schema that could include a type (e.g. “dialogue” vs “internal”) and parameters. The active inference planner can create an Action (like “say X to the user” or “update internal belief Y”) and evaluate its predicted outcome before committing it. NyxBrain’s external API (like generate_response) would then simply execute the chosen Action (if it’s a dialogue, format it into text). This separation makes it easier to manage non-dialogue actions too (if, say, Nyx needed to manipulate some internal state or trigger an event in the story world, it could be represented similarly).
4. Module Interface and Message Passing
Align Module Interfaces with Object Messages: Each cognitive subsystem (memory, emotion, planning, etc.) should be adapted to use the world model as the common language. For example, the memory module in an Axiom-enhanced Nyx might subscribe to changes in the world state (via the context system) and automatically store significant events or retrieve relevant facts when needed, rather than waiting for an explicit function call. The existing ContextDistributionSystem can be leveraged heavily here:
Continue to use SharedContext to post state updates, but evolve the content of those updates. Instead of opaque data or just text, include structured object info. (The SharedContext is a Pydantic model, so you could even embed object references or IDs in it.) For instance, when the perception module updates the user’s emotional_state, it could add an update like update_type="emotion_update", data={"user_id": X, "mood": "sad"} which the emotion module and others see.
Modules like the Goal System or Theory-of-Mind should similarly broadcast changes (e.g. “inferred user belief updated”) so that the world model and any interested module stay in sync.
You might need to expand the context system to handle bi-directional linking with the world model. One approach is to make the World Model itself one of the “modules” in the context distribution network – i.e., treat it as the global blackboard state that gets updated by other modules and also sends out derived updates. In code, this could mean giving the world model class methods to apply ContextUpdate messages (for updating object state) and to emit ContextUpdate when an object’s state changes. The existing ContextAwareModule base class
GitHub
GitHub
 can guide how to structure modules to receive and send context; you would likely subclass this for any new Axiom-related modules. Reduce Direct Calls in Favor of Events: Currently, NyxBrain calls methods on subsystems (e.g. brain.memory_orchestrator.retrieve_memories() as seen in the _retrieve_memories_tool implementation
GitHub
). After integration, you can gradually shift these to an event-driven mechanism. For example, instead of directly invoking retrieve, the brain could create a context update like update_type="memory_query", data={"query": ..., "result_key": "latest_memory_results"} and post it. The memory module, upon seeing this, performs the search and posts back an update with the results. This decoupling improves modularity and mirrors how an active inference agent might behave (post a question to the “world” and get an answer), rather than imperative function calls. It also eases swapping out modules – e.g., you could replace Nyx’s memory module with Axiom’s memory system without changing the Brain’s logic, as long as it responds to the same messages.
5. Adapting NyxBrain Integration Points
To implement the above changes, focus on key integration points in code:
NyxBrain Initialization (nyx/core/brain/base.py): Modify the NyxBrain.initialize() method to set up the Axiom-based infrastructure. This is analogous to how the Global Workspace integration suggests instantiating modules and engine
GitHub
GitHub
. If Axiom comes with its own engine or runtime class (say AxiomEngine), you would initialize it here with references to the world model and any Axiom modules. For example:
python
Copy
Edit
# Pseudo-code for NyxBrain.initialize after Axiom integration
async def initialize(self):
    # Initialize world model (object store)
    self.world = WorldModel()
    # Set up subsystems with references to world model
    self.memory_module = AxiomMemoryModule(self.world)
    self.emotion_module = AxiomEmotionModule(self.world)
    self.planning_module = AxiomPlanningModule(self.world)
    # ... instantiate other modules or reuse Nyx's where possible
    # Initialize context distribution or Axiom engine
    self.context_system = ContextDistributionSystem(self)
    # If using AxiomEngine that manages active loop:
    self.engine = AxiomEngine(self.world, modules=[...], context_system=self.context_system)
    await self.engine.start()
This is just illustrative – the key point is to attach a world model and replace/augment Nyx’s subsystems with ones aware of that model. NyxBrain might still keep some existing attributes (for compatibility), but many (like memory_orchestrator, emotional_core) could be moved inside the Axiom engine or world model.
NyxBrain Process Flow: Update NyxBrain.process_input() to leverage the new system. In the global workspace example, after wiring up, process_input simply handed off to the engine and wrapped the result
GitHub
. You can do similarly: first pass the input to the Perception module to update the world model, then let the Axiom engine run one or more inference cycles, and finally collect the decided action. For instance:
python
Copy
Edit
async def process_input(self, user_input: str) -> Dict[str, Any]:
    if not self.engine:
        await self.initialize()
    # Perception: update world state
    await self.engine.perception_module.handle_input(user_input)
    # Trigger active inference/planning cycle for one iteration
    decision = await self.engine.process_input(user_input)
    # decision might be an Action object or message string
    return {"message": str(decision), "engine": "AxiomAI"}
This ensures that every input updates the internal state and then yields a response based on that updated state, rather than just feeding text to an LLM. The snippet above is analogous to the global workspace starter kit (where NyxEngine.process_input was called and its result packaged
GitHub
).
Subsystem Bridges: During integration, provide bridges between old and new systems. For example:
If some parts of Nyx are still using brain.memory_orchestrator, have that orchestrator internally call the Axiom memory module or world model (as mentioned earlier for retrieve/add memory functions).
The Goal System (goal_system.py) might continue to operate, but you can map its goal checks into the world model. For instance, if a goal step says action: "generate_response", you can implement that by invoking the planning module or engine to get a response rather than directly using the old pipeline.
The Theory-of-Mind model can be integrated by linking the UserMentalState into the user’s object in the world model (so that updates from ToM become updates to that object’s attributes). This might involve minor adjustments in theory_of_mind.py – for example, when it infers a new belief, instead of storing it only in its internal user_models dict, also propagate it to the world model’s representation of that user. You could utilize the context distribution (send a context update of type "belief_update") to achieve this.
Full Replacement vs Partial Hybrid Integration
Full Replacement Approach: This would mean rebuilding Nyx’s “brain” entirely around Axiom’s architecture, effectively swapping out NyxBrain for a new AxiomBrain that implements object-centric active inference from the ground up. In practice, you’d discard or heavily refactor nyx/core/brain/base.py and many subsystems:
The memory system would be replaced by Axiom’s memory/knowledge base.
The multi-agent processing (Nyx’s AgentProcessor with main/creative/analytical agents) might be removed in favor of a single coherent reasoning module (or whatever Axiom prescribes, perhaps one policy model rather than three persona agents).
All coordination would be handled by Axiom’s message passing or blackboard, possibly making Nyx’s context distribution and global workspace code redundant.
Pros: A clean-slate implementation could be more efficient and consistent with Axiom principles. It avoids the complexity of maintaining backward-compatible shims. If Nyx’s current design has technical debt or ad-hoc fixes, a full rewrite could streamline the architecture (e.g., no need for the memory orchestrator agent if Axiom’s approach can replace it with algorithmic logic). It also ensures that all aspects (memory, goal selection, perception) adhere to object-centric active inference, maximizing the benefits. Cons: This is high risk and high effort. Nyx is a large system with many domain-specific behaviors (for instance, the Dominance module, safety guardrails, etc.), and reimplementing all of these in a new paradigm could introduce regressions. There’s a chance of losing subtle features – for example, Nyx’s finely-tuned prompt style for the Creative vs Analytical agents might be lost if replaced by a single reasoning model, possibly affecting the quality of storytelling. Full replacement also means debugging an entirely new brain while the old one is thrown out, which could stall development for a while. Given that Nyx has recently added things like global workspace integration in small steps, a big-bang rewrite might be too disruptive. Hybrid/Incremental Integration (Recommended): A more prudent approach is to gradually integrate Axiom’s concepts, creating a hybrid system that can evolve into a full Axiom-based architecture over time:
Phase 1 – Parallel World Model: Introduce the new world model and start feeding it data, while still using the existing NyxBrain logic for primary responses. For instance, on each user input, run the new Perception module to update the world model (in parallel to the current processing). You can log or evaluate what the Axiom-based planner would do without yet replacing the actual output. This phase flushes out how well the object-centric understanding works on live data.
Phase 2 – Subsystem Swaps: One by one, swap Nyx subsystems with Axiom implementations. You might start with memory, since that’s a self-contained piece: implement an Axiom memory module that perhaps uses a graph database or an improved vector store keyed by objects, and switch NyxBrain to use it for retrieval queries (keeping the old memory as backup). Next, you could integrate an Axiom planner to handle goal selection – for example, intercept calls to Nyx’s goal executor and let the Axiom planner suggest the next step (or use Axiom’s active inference to drive goal completion, while still monitoring with Nyx’s goal system for consistency).
Phase 3 – Core Loop Integration: Once individual pieces are validated, change NyxBrain’s main loop to fully delegate to the Axiom engine (similar to the global workspace delegation code). At this stage, process_input essentially hands control to the object-centric active-inference engine and returns whatever it outputs
GitHub
. NyxBrain becomes mostly a wrapper that initializes and holds the Axiom engine. The legacy code paths (Nyx’s AgentProcessor, etc.) can be kept toggled off or as fallback modes.
Phase 4 – Cleanup: Remove or refactor remnants of the old architecture that are no longer used. For example, if the Axiom engine handles cross-module messaging internally, you might retire context_distribution.py or simplify it. Or if all memory queries are answered by the new world model, the MemoryCoreAgents and orchestrator code can be removed or kept only for historical data migration. Also at this stage, any domain-specific modules (like the Dominance content generator in dominance.py) should be refactored to work as object-centric interactions (e.g., the “FemdomActivityIdea” could become an object or a structured action in the world model, rather than being formed through pure text generation).
Why Hybrid: This approach lowers risk by ensuring Nyx always has a working brain during the transition. You can test Axiom components side-by-side with the existing ones and incrementally gain confidence. It also allows for partial rollbacks; for example, if an Axiom-based planning strategy isn’t producing better results, you can temporarily fall back on Nyx’s original planning logic (since it’s still there) while tweaking the new one.
Implementation Guidance and File References
To make the discussion more concrete, here are specific recommendations with references to Nyx’s code structure:
Establish the World Model Class: Create a module (e.g., nyx/core/world_model.py) that defines core object types. You might start with a base Entity class (with a unique ID and type field) and subclasses or specialized fields for different kinds of entities (User/Character, Location, etc.). This class can also contain methods like update_attribute(name, value) and internal logging of changes (which could emit context updates). Since Nyx already uses Pydantic models extensively for data (Memory, Emotions, etc.), you can use Pydantic for these Entity models too, which makes validation and serialization easier. The World Model might also contain a KnowledgeGraph or relationship mapping if needed. It should have methods to query objects by ID or name.
Inject World Model into NyxBrain: In nyx/core/brain/base.py, when instantiating NyxBrain, add an attribute for the world model (self.world = WorldModel()). Then, ensure subsystems have access to it. For instance, pass self.world to the Memory orchestrator or emotional core if they need it. The Global Workspace snippet in starter_kit.md shows adding modules like MemoryModule(None) to an engine
GitHub
 – in your case, you might do MemoryModule(self.world) if the module needs the world reference. By citing that snippet, we see how the engine is composed and how the brain holds it
GitHub
.
Revise Memory Integration: Focus on nyx/core/memory_core.py and memory_orchestrator.py. The goal is to pivot from free-text memory to structured knowledge:
In MemoryCoreAgents.add_memory (async method that adds a memory)
GitHub
GitHub
, instead of just writing to the MemoryStorage, also update the world model. For example, if the memory has entities=["dragon"] and text “The dragon gave me a quest”, find or create the dragon object in self.world and attach this info (maybe as an entry in a dragon.memories list or set dragon.last_action = "gave quest to user" etc.). This means the next time the agent needs to recall something about the dragon, it could just check the dragon object’s state instead of searching through all memory text.
Use the entity indices that Nyx already maintains
GitHub
GitHub
to populate object relationships. For instance, MemoryStorage.entity_index maps each entity to memory IDs
GitHub
; you can iterate over that to build an initial property like Entity.related_memories.
If Axiom provides a specific way to handle semantic memory (object property learning vs episodic memory), align your implementation with that. You might find that some memory entries become object attributes (e.g., a memory “Alice’s hair is blonde” is not just an event but a permanent attribute of object Alice), whereas others remain as episodic events (e.g., “Yesterday Alice fought a goblin” is a time-stamped event linked to Alice and goblin objects).
Continue to support the existing memory query interface during transition. For instance, the _retrieve_memories_tool currently calls brain.memory_orchestrator.retrieve_memories(...) and returns a list of memory dicts
GitHub
. You can implement retrieve_memories to first try the new world model for answers. If the query is something like a semantic question (“Who gave me a quest?”), the world model might answer directly (“the dragon”) by looking at objects; if it’s more vague (“what happened recently?”), it could fall back to scanning recent MemoryStorage entries.
Leverage ContextDistribution and Global Workspace: The presence of ContextDistributionSystem is advantageous – it’s essentially a pub-sub hub already integrated with NyxBrain. Rather than building a new message bus from scratch, use this system to handle communications between your new modules:
Register the new modules as context-aware. For example, if you implement a WorldModelModule that inherits from ContextAwareModule, NyxBrain (or the Axiom engine) can register it so it will receive context on each new interaction. You might override process_input in that module to do nothing (since world model updates happen during perception), but override receive_context_update to catch any updates and apply them to the object store.
Use ContextUpdate dataclass
GitHub
 for broadcasting changes. For instance, when the planning module decides on an action, it could create a ContextUpdate(source_module="planner", update_type="proposed_action", data={...}) which the NyxBrain or output module picks up to actually execute (speak to user).
The Global Workspace architecture code (in nyx/core/global_workspace/global_workspace_architecture.py) likely contains patterns for consensus or attention between modules. You might not use it directly if Axiom has a different mechanism, but it’s worth reviewing to inspire how to structure multi-module decision making. (E.g., global workspace might have an AttentionMechanism that selects which module’s info gets broadcast widely – in active inference, attention could correspond to weighting certain prediction errors more, etc.)
Example Code Reference – Delegation to Engine: To illustrate how your NyxBrain will delegate to a new engine (Axiom or otherwise), consider the global workspace integration example. In the snippet below, NyxBrain’s process_input simply calls the engine and returns its output:
python
Copy
Edit
# nyx/core/brain/base.py
async def process_input(self, user_input: str) -> dict[str, any]:
    if self.engine is None:
        await self.initialize()
    decision = await self.engine.process_input(user_input)
    return {
        "message": decision,
        "engine": "GWA",
    }
GitHub
.
For Axiom, you would do something similar – except the engine might produce more than just a message (maybe an action object or a tuple of action and expected outcome). You can wrap that into Nyx’s response schema (ResponseResult) as needed
GitHub
GitHub
. For instance, "engine": "AxiomAI" as a marker, or if you have an object result, convert it to a user-facing message.
Safety and Reflexes: Nyx has input guardrails and reflex systems (e.g., a Safety Checker agent that can veto unsafe input
GitHub
GitHub
). Ensure these are re-integrated in the new framework. In an object-centric design, even safety could be treated as an object (like a “moderation” object that evaluates content) or as a monitoring process. You might keep the existing guardrail functions as is, but just call them at appropriate points (for example, after perception, run the safety check on the parsed input to decide if the agent should refuse).
Domain-Specific Modules: Don’t forget to update specialized modules like the Dominance system (nyx/core/dominance.py) or others (e.g., “needs” or “orgasm control” if present under a2a/). These likely rely on the old approach (lots of text prompts and isolated context). With Axiom integration, these can be reframed as either special objects (e.g., a “DominanceScenario” object that tracks state of that storyline, including trust levels, limits, etc., which are currently in the DominanceContext
GitHub
GitHub
) or specialized modules that subscribe to relevant context (e.g., a module that listens for changes in user’s submission level or emotional state to adjust the dominance suggestions). The aim is to keep their functionality but have them operate on the shared world state rather than in a vacuum.
Conclusion and Recommendation
Recommendation: Proceed with a partial integration strategy, gradually refactoring Nyx’s AI architecture to incorporate Axiom’s principles, rather than an abrupt full replacement. This hybrid approach will allow Nyx to become more efficient, structured, and intelligent over time, while preserving functionality during the transition. Concretely:
Start by integrating an Axiom-style world model and active inference engine alongside NyxBrain, using Nyx’s context distribution to bind them together. For instance, embed an Axiom engine in NyxBrain (similar to the NyxEngine from global workspace
GitHub
) and have it mirror the processing of inputs in parallel to the existing pipeline. This will expose gaps and necessary adaptations (like additional object types or inference rules needed for the domain).
Incrementally replace Nyx subsystems with Axiom modules: Swap out memory retrieval logic to use structured queries (improving speed and reducing reliance on GPT for every lookup), adopt the object-centric representation for user state and emotions (so Theory-of-Mind updates go directly into the user’s object profile), and introduce a planning mechanism that uses the world model to simulate outcomes. Each replacement can be tested in isolation. For example, after integrating Axiom’s memory, you should observe that NyxBrain is able to fetch facts more directly (check that brain.memory_orchestrator.retrieve_memories now returns results via the new system by, say, querying the entity index or properties
GitHub
 rather than always invoking the LLM agent).
Use a hybrid loop to gradually move to active inference: Perhaps alternate between the old response generation and a new Axiom-based response. Initially, you might run Axiom’s planning and get a “suggested answer” and compare it with the legacy answer for a few iterations (not visible to the user, just for validation). Once the Axiom engine consistently produces equal or better outputs, you flip the switch to let it drive the conversation fully.
Preserve external behavior and APIs: Ensure that from an outside perspective, NyxBrain still responds to inputs and yields a ResponseResult in the same format (so that the web UI or game logic around it doesn’t break). Internally, you can change a lot, but externally NyxBrain might still have methods process_input(user_text) and generate_response(...) that work as before (they just now delegate to the new mechanisms). This way, integration is seamless to the rest of the system.
In doing all this, you’ll transform Nyx from a somewhat loosely coupled collection of prompt-engineered agents into a more unified cognitive architecture. The object-centric world model will allow for more coherent reasoning (the AI will “know” facts as properties it can reference, not have to rediscover them via text each time), and the active inference loop will make the AI’s behavior more purposeful and adaptive (since it’s always evaluating “where do I expect the story/state to go, and how can I influence it?”). Ultimately, a full Axiom-based NyxBrain may emerge, but by using a phased integration, you significantly de-risk the process. You preserve Nyx’s rich feature set (memory, emotions, goals, narrative crafting) while reorganizing them under Axiom’s principles. The end result should be an AI system that is easier to maintain and extend (thanks to clearer object-centric state and module interfaces), more efficient (fewer redundant LLM calls by using explicit state where possible), and more intelligent in its interactions (able to plan and infer like a proactive agent rather than just react). By following the code adaptation steps above – updating key files in nyx/core like brain/base.py for engine wiring
GitHub
, memory_core.py for structured memory, and using the context distribution for integration – you can iteratively evolve Nyx into the Axiom paradigm.
